<html>
<head>
<title>segmentation.ipynb</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #8c8c8c; font-style: italic;}
.s1 { color: #080808;}
.s2 { color: #0033b3;}
.s3 { color: #067d17;}
.s4 { color: #1750eb;}
.s5 { color: #0037a6;}
</style>
</head>
<body bgcolor="#ffffff">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#c0c0c0" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
segmentation.ipynb</font>
</center></td></tr></table>
<pre><span class="s0">#%% md 
</span><span class="s1"># **MAREPIU - AN ANALYSIS OF B2C MARKET OPPORTUNITIES** 
 
Marepiù is a well-established company in the seafood industry, primarily operating in the B2B (wholesale) market. This study was conducted to analyze the market and evaluate the feasibility of entering the B2C sector, with the aim of understanding consumer preferences and needs. To achieve this, a consumer survey was conducted to gather data on seafood consumption patterns, shopping habits, and demographic characteristics. The collected data was analyzed using statistical models to identify key market trends and consumer expectations. The findings of this analysis will provide crucial support for the company’s strategic decisions, helping to determine whether—and how—Marepiù’s seafood products can be successfully introduced to the retail sector. 
 
--- 
## Table of Contents 
 
### 1. Data Loading and Description 
 
### 2. Segmentation 
- 2.1. Data 
- 2.2. Clustering 
  - 2.2.1. Optimal Number of Clusters 
  - 2.2.2. K-Means (k=2) 
  - 2.2.3. K-Means (k=4) 
  - 2.2.4. Hierarchical Clustering 
- 2.3. PCA 
  - 2.3.1. Principal Components 
  - 2.3.2. Optimal Number of Clusters 
  - 2.3.3. K-Means (k=4) 
  - 2.3.4. Clusters Analysis (k=2) 
- 2.4. Variable Selection 
  - 2.4.1. Optimal Number of Clusters 
  - 2.4.2. K-Means (k=2) 
  - 2.4.3. K-Means (k=4) 
- 2.5. Clusters Analysis 
  - 2.5.1. Analysis of Sample 
  - 2.5.2. Analysis of Clusters 
 
### 3. Classification 
- 3.1. Multinomial Logit Model 
- 3.2. Random Forest 
 
### 4. Conclusions and Limitations 
--- 
 
</span><span class="s0">#%% md 
</span><span class="s1"># **SEGMENTATION** 
A first general questionnaire was conducted to analyze the overall consumer market and identify potential target segments for the company’s entry into the B2C market.  
The survey aimed to gather insights into consumer habits, behaviors, and needs within the seafood category, serving as a preliminary step before narrowing the focus on specific products or attributes. 
To ensure comprehensive data collection, the survey was structured with no optional questions, avoiding any missing values. Additionally, an “Other: …” answer was included in every multiple-choice question to allow respondents to express ideas not covered by the predefined options. 
The questionnaire was structured as follows:  
 
**1. Demographic Data**: general questions about demographic characteristics (e.g., age, gender, geographical location) were included to profile consumers. 
 
**2. Consumption Habits**: this section explored frequency of seafood consumption, types of seafood most frequently consumed (e.g., fresh, frozen, or processed), average time spent on meal preparation during the day. 
From these responses, potential interest in frozen seafood products can be hypothesized. For instance: 
 
-   Consumers who spend significant time preparing meals might not feel the need for ready-to-cook frozen alternatives. 
 
-   On the other hand, consumers with less time for meal preparation might represent a more receptive audience for frozen seafood products. 
 
**3. Purchasing Behavior**: this section explored shopping preferences and habits (e.g., purchasing channels, price consumers are willing to spend). 
These questions aimed to uncover actionable information about the most appropriate distribution strategies for frozen seafood products. 
 
**4. Product Preferences**: questions about key product attributes such as freshness, nutritional balance, price, speed and simplicity of preparation, were designed to assess the attractiveness of the company’s potential offerings in the consumer market and identify which product features resonate most with the target audience.  
 
Analyzing the responses provides valuable insights into the seafood market, enabling the identification of key trends and effective audience segmentation. This first general survey sets the stage for more focused studies (e.g., conjoint analysis) by highlighting critical factors like the importance of convenience or the role of speed and simplicity of cooking in consumer decisions. 
 
The survey was distributed through Google Forms and shared via WhatsApp, Facebook groups, university groups, and Instagram, reaching family, friends, acquaintances, and colleagues. We concluded with a total of **347 respondents** and **28 questions**. 
</span><span class="s0">#%% 
</span><span class="s2">import </span><span class="s1">pandas </span><span class="s2">as </span><span class="s1">pd</span>
<span class="s2">import </span><span class="s1">matplotlib.pyplot </span><span class="s2">as </span><span class="s1">plt</span>
<span class="s2">from </span><span class="s1">sklearn.cluster </span><span class="s2">import </span><span class="s1">KMeans</span>
<span class="s2">from </span><span class="s1">yellowbrick.cluster </span><span class="s2">import </span><span class="s1">KElbowVisualizer</span>
<span class="s2">from </span><span class="s1">scipy </span><span class="s2">import </span><span class="s1">stats</span>
<span class="s2">from </span><span class="s1">statsmodels.stats </span><span class="s2">import </span><span class="s1">weightstats </span><span class="s2">as </span><span class="s1">st</span>
<span class="s2">import </span><span class="s1">numpy </span><span class="s2">as </span><span class="s1">np</span>
<span class="s2">import </span><span class="s1">seaborn </span><span class="s2">as </span><span class="s1">sns</span>
<span class="s2">from </span><span class="s1">sklearn.decomposition </span><span class="s2">import </span><span class="s1">PCA</span>
<span class="s2">from </span><span class="s1">sklearn.preprocessing </span><span class="s2">import </span><span class="s1">StandardScaler</span>
<span class="s2">from </span><span class="s1">scipy.cluster.hierarchy </span><span class="s2">import </span><span class="s1">linkage, dendrogram, fcluster</span>
<span class="s2">from </span><span class="s1">sklearn.metrics </span><span class="s2">import </span><span class="s1">silhouette_samples, silhouette_score</span>
<span class="s2">from </span><span class="s1">factor_analyzer.factor_analyzer </span><span class="s2">import </span><span class="s1">calculate_kmo</span>
<span class="s2">import </span><span class="s1">re</span>
<span class="s2">import </span><span class="s1">statsmodels.formula.api </span><span class="s2">as </span><span class="s1">smf</span>
<span class="s2">from </span><span class="s1">sklearn.metrics </span><span class="s2">import </span><span class="s1">ConfusionMatrixDisplay</span>
<span class="s2">from </span><span class="s1">statsmodels.stats.outliers_influence </span><span class="s2">import </span><span class="s1">variance_inflation_factor</span>
<span class="s2">from </span><span class="s1">sklearn.ensemble </span><span class="s2">import </span><span class="s1">RandomForestClassifier</span>
<span class="s2">from </span><span class="s1">sklearn.model_selection </span><span class="s2">import </span><span class="s1">train_test_split</span>
<span class="s2">from </span><span class="s1">sklearn.metrics </span><span class="s2">import </span><span class="s1">accuracy_score, classification_report</span>
<span class="s2">from </span><span class="s1">sklearn.preprocessing </span><span class="s2">import </span><span class="s1">OneHotEncoder</span>
<span class="s0">#%% md 
</span><span class="s1">--- 
 
 
## **1. Data loading and description** 
The dataset consists of 347 rows and 28 columns, each representing a specific question from the survey. 
</span><span class="s0">#%% 
</span><span class="s1">raw_data = pd.read_csv(</span><span class="s3">&quot;data/segmentation_data.csv&quot;</span><span class="s1">)</span>
<span class="s1">raw_data.head()</span>
<span class="s0">#%% md 
</span><span class="s1">After collecting general demographic information, such as age range or region of residence, the survey included an initial question: &quot;*Do you consume fish products*?&quot;. Respondents who selected &quot;*NO*&quot; were redirected to the final section of the survey, in which they were asked for motivations. 
</span><span class="s0">#%% 
</span><span class="s1">data_no = raw_data[raw_data[</span><span class="s3">'Consumi prodotti ittici?'</span><span class="s1">]==</span><span class="s3">'No'</span><span class="s1">]</span>
<span class="s1">len(data_no)</span>
<span class="s0">#%% 
</span><span class="s1">no_fish = len(data_no)/len(raw_data)*</span><span class="s4">100</span>
<span class="s1">print(</span><span class="s3">f&quot;Percentage of people who don't eat fish (according to our data) is </span><span class="s5">{</span><span class="s1">no_fish</span><span class="s5">:</span><span class="s3">.2f</span><span class="s5">}</span><span class="s3">%&quot;</span><span class="s1">)</span>
<span class="s0">#%% md 
</span><span class="s1">While we acknowledge that our sample is not perfectly representative of the general population, we can infer, based on the collected data, that approximately **one-tenth of the population could be excluded a priori** as potential customers. This is due to their complete abstention from consuming fish products, regardless of influencing factors such as price, freshness, or availability. This insight is valuable for refining target market strategies and focusing resources on segments with higher potential. 
 
To strengthen this analysis and validate the assumption, we investigated the specific reasons behind this group’s decision to avoid fish products. Understanding these motivations provides a more comprehensive picture of the consumer landscape and offers indirect insights into potential barriers or opportunities for expanding market reach. By analyzing these responses, we aim to identify whether their abstention is driven by personal preferences, dietary restrictions, ethical considerations, or other factors, thus enriching the overall interpretation of the sample under study. 
</span><span class="s0">#%% 
</span><span class="s1">response_distribution = raw_data[</span><span class="s3">'Per quale motivo non consumi prodotti ittici?'</span><span class="s1">].value_counts()</span>
<span class="s1">response_distribution</span>
<span class="s0">#%% md 
</span><span class="s1">------------------------------------------------------------------------------------------------------------------------------ 
</span><span class="s0">#%% md 
</span><span class="s1">After this brief preliminary analysis, we filtered the data to include only respondents who consume seafood products. 
</span><span class="s0">#%% 
</span><span class="s1">data = raw_data[raw_data[</span><span class="s3">'Consumi prodotti ittici?'</span><span class="s1">]==</span><span class="s3">'Si'</span><span class="s1">]</span>
<span class="s1">data.head()</span>
<span class="s0">#%% 
</span><span class="s1">print(</span><span class="s3">f&quot;Number of people who eat fish products: </span><span class="s5">{</span><span class="s1">len(data)</span><span class="s5">}</span><span class="s3">&quot;</span><span class="s1">)</span>
<span class="s1">print(</span><span class="s3">f&quot;Percentage: </span><span class="s5">{</span><span class="s1">len(data)/len(raw_data)*</span><span class="s4">100</span><span class="s5">:</span><span class="s3">.2f</span><span class="s5">}</span><span class="s3">%&quot;</span><span class="s1">)</span>
<span class="s0">#%% md 
</span><span class="s1">**308 people eat fish**, which is almost 90% of our sample. Of these, we want to understand their needs and habits in order to identify similar behaviors within groups of the population. 
</span><span class="s0">#%% md 
</span><span class="s1">To conclude data pre-processing, we set an index column corresponding to **respondent ID**. 
</span><span class="s0">#%% 
</span><span class="s1">data = data.copy()</span>
<span class="s1">data.loc[:, </span><span class="s3">'respondent_id'</span><span class="s1">] = data.index</span>
<span class="s1">data = data.set_index(</span><span class="s3">'respondent_id'</span><span class="s1">)</span>
<span class="s0">#%% md 
</span><span class="s1">--- 
 
 
## **2. Segmentation** 
</span><span class="s0">#%% md 
</span><span class="s1">### **2.1. Data** 
The dataframe &quot;_segmentation_data_&quot; consists of a selection of columns (17 to 28) from the original dataset. They contain the importance ratings assigned from each respondent to various product characteristics. The attributes we decided to take into account are the following: 
- **freshness**: importance of the freshness of the purchased product. 
- **origin**: importance of the origin of the raw materials. 
- **cooking_speed**: importance of the speed of preparation. 
- **cooking_simplicity**: importance of the simplicity of preparation. 
- **nutritional_balance**: importance of the nutritional balance of the product. 
- **innovative_products**: importance of the presence of new/innovative products. 
- **unprocessed**: importance of the product being natural/minimally processed. 
- **price**: importance of the price. 
- **single_portion**: importance of the availability of single-portion products. 
- **sustainable_sources**: importance of the ingredients coming from sustainable sources (e.g., certified fishing). 
- **low_impact_environment**: importance of low environmental impact in production. 
- **sustainable_packaging**: importance of eco-friendly packaging. 
 
The responses range from 1 to 5, where 1 represents &quot;Not interested&quot; and 5 represents &quot;Very important&quot;. 
</span><span class="s0">#%% 
</span><span class="s1">segmentation_data = data.iloc[:, </span><span class="s4">16</span><span class="s1">:</span><span class="s4">28</span><span class="s1">]</span>
<span class="s1">segmentation_data</span>
<span class="s0">#%% md 
</span><span class="s1">We renamed columns with pre-specified and easier **column names** and mapped cell values to integers. 
</span><span class="s0">#%% 
# rename columns</span>
<span class="s1">new_names = { </span><span class="s3">'Da 1 a 5, quanto sono importanti per te le seguenti caratteristiche? [La freschezza del prodotto acquistato]' </span><span class="s1">: </span><span class="s3">'freshness'</span><span class="s1">,</span>
       <span class="s3">'Da 1 a 5, quanto sono importanti per te le seguenti caratteristiche? [La provenienza della materia prima]'</span><span class="s1">: </span><span class="s3">'origin'</span><span class="s1">,</span>
       <span class="s3">'Da 1 a 5, quanto sono importanti per te le seguenti caratteristiche? [La velocità di preparazione]' </span><span class="s1">: </span><span class="s3">'cooking_speed'</span><span class="s1">,</span>
       <span class="s3">'Da 1 a 5, quanto sono importanti per te le seguenti caratteristiche? [La semplicità di preparazione]' </span><span class="s1">: </span><span class="s3">'cooking_simplicity'</span><span class="s1">,</span>
       <span class="s3">'Da 1 a 5, quanto sono importanti per te le seguenti caratteristiche? [Il bilancio nutrizionale del prodotto]' </span><span class="s1">: </span><span class="s3">'nutritional_balance'</span><span class="s1">,</span>
       <span class="s3">'Da 1 a 5, quanto sono importanti per te le seguenti caratteristiche? [La presenza di prodotti nuovi/innovativi]'</span><span class="s1">: </span><span class="s3">'innovative_products'</span><span class="s1">,</span>
       <span class="s3">'Da 1 a 5, quanto sono importanti per te le seguenti caratteristiche? [Che il prodotto sia al naturale/poco lavorato]' </span><span class="s1">: </span><span class="s3">'unprocessed'</span><span class="s1">,</span>
       <span class="s3">'Da 1 a 5, quanto sono importanti per te le seguenti caratteristiche? [Il prezzo]' </span><span class="s1">: </span><span class="s3">'price'</span><span class="s1">,</span>
       <span class="s3">'Da 1 a 5, quanto sono importanti per te le seguenti caratteristiche? [La presenza di prodotti monoporzione]' </span><span class="s1">: </span><span class="s3">'single_portion'</span><span class="s1">,</span>
       <span class="s3">'Da 1 a 5, quanto sono importanti per te le seguenti caratteristiche in termini di sostenibilità? [La provenienza da fonti sostenibili degli ingredienti (es. pesca certificata)]' </span><span class="s1">: </span><span class="s3">'sustainable_sources'</span><span class="s1">,</span>
       <span class="s3">'Da 1 a 5, quanto sono importanti per te le seguenti caratteristiche in termini di sostenibilità? [Il basso impatto ambientale della produzione]' </span><span class="s1">: </span><span class="s3">'low_impact_environment'</span><span class="s1">,</span>
       <span class="s3">'Da 1 a 5, quanto sono importanti per te le seguenti caratteristiche in termini di sostenibilità? [Il packaging eco-sostenibile ]' </span><span class="s1">: </span><span class="s3">'sustainable_packaging'</span><span class="s1">}</span>

<span class="s1">segmentation_data = segmentation_data.rename(columns=new_names)</span>

<span class="s0"># map textual responses to numerical values</span>
<span class="s1">response_mapping = {</span>
    <span class="s3">'1 - Non mi interessa'</span><span class="s1">: </span><span class="s4">1</span><span class="s1">,</span>
    <span class="s3">'2 - Poco importante'</span><span class="s1">: </span><span class="s4">2</span><span class="s1">,</span>
    <span class="s3">'3 - Mediamente importante'</span><span class="s1">: </span><span class="s4">3</span><span class="s1">,</span>
    <span class="s3">'4 - Abbastanza importante'</span><span class="s1">: </span><span class="s4">4</span><span class="s1">,</span>
    <span class="s3">'5 - Molto importante'</span><span class="s1">: </span><span class="s4">5</span>
<span class="s1">}</span>

<span class="s2">for </span><span class="s1">column </span><span class="s2">in </span><span class="s1">segmentation_data.columns:</span>
    <span class="s1">segmentation_data[column] = segmentation_data[column].map(response_mapping)</span>

<span class="s1">segmentation_data.head()</span>
<span class="s0">#%% md 
</span><span class="s1">Check **null values**: 
</span><span class="s0">#%% 
</span><span class="s1">segmentation_data.info()</span>
<span class="s0">#%% md 
</span><span class="s1">We computed some **metrics** of each attribute ... 
</span><span class="s0">#%% 
</span><span class="s1">segmentation_data.describe().style.format(</span><span class="s3">&quot;{:.1f}&quot;</span><span class="s1">)</span>
<span class="s0">#%% md 
</span><span class="s1">... and **plotted** distribution to understand. 
</span><span class="s0">#%% 
</span><span class="s1">num_variables = len(segmentation_data.columns) </span>
<span class="s1">num_rows = num_variables // </span><span class="s4">3 </span><span class="s1">+ (num_variables % </span><span class="s4">3 </span><span class="s1">&gt; </span><span class="s4">0</span><span class="s1">)</span>

<span class="s1">fig, axes = plt.subplots(nrows=num_rows, ncols=</span><span class="s4">3</span><span class="s1">, figsize=(</span><span class="s4">15</span><span class="s1">, num_rows * </span><span class="s4">4</span><span class="s1">), tight_layout=</span><span class="s2">True</span><span class="s1">)</span>
<span class="s1">axes = axes.flatten()</span>

<span class="s2">for </span><span class="s1">index, column </span><span class="s2">in </span><span class="s1">enumerate(segmentation_data.columns):</span>
    <span class="s1">axes[index].hist(segmentation_data[column], bins=</span><span class="s4">5</span><span class="s1">, range=(</span><span class="s4">1</span><span class="s1">, </span><span class="s4">5</span><span class="s1">), alpha=</span><span class="s4">0.7</span><span class="s1">, color=</span><span class="s3">'blue'</span><span class="s1">, edgecolor=</span><span class="s3">'black'</span><span class="s1">)</span>
    <span class="s1">axes[index].set_title(column)</span>
    <span class="s1">axes[index].set_xlabel(</span><span class="s3">'Rating'</span><span class="s1">)</span>
    <span class="s1">axes[index].set_ylabel(</span><span class="s3">'Frequency'</span><span class="s1">)</span>
    <span class="s1">axes[index].set_xticks(range(</span><span class="s4">1</span><span class="s1">, </span><span class="s4">6</span><span class="s1">))</span>

<span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(index + </span><span class="s4">1</span><span class="s1">, len(axes)):</span>
    <span class="s1">axes[i].axis(</span><span class="s3">'off'</span><span class="s1">)</span>

<span class="s1">plt.show()</span>
<span class="s0">#%% md 
</span><span class="s1">The distributions reveal insights into the **respondents' perceptions and preferences** across different attributes. Overall, many attributes are skewed towards higher ratings (4 and 5), indicating that most respondents seem to have positive views on the various characteristics. However, there are some notable exceptions where responses are more balanced or spread across lower ratings, reflecting some areas where satisfaction may vary more. 
</span><span class="s0">#%% md 
</span><span class="s1">A brief analysis of each attribute was conducted: 
- **Freshness:** strong preference for high freshness, with the majority of respondents rating it 4 or 5, suggesting that freshness is a crucial factor for seafood consumers. 
- **Origin:** similar to freshness, a significant portion of the respondents rated the origin of seafood positively (4 and 5), but there are more people rating it lower (1 and 2). 
- **Cooking Speed:** responses are more spread across the rating scale, with the highest concentration in the mid-range. 
- **Cooking Simplicity:** the distribution is more positively skewed with a high number of respondents rating simplicity at 4. 
- **Nutritional Balance:** many respondents rating it 4 or 5. 
- **Innovative Products:** the distribution is more balanced, with the highest frequencies at 2 and 3, indicating a moderate interest in innovative products. This may indicate that while innovation in seafood products is appreciated, it might not be a top priority for most respondents. 
- **Unprocessed:** there is a strong preference for unprocessed seafood, with a high concentration of ratings at 4 and 5. 
- **Price:** the distribution is right-skewed, with the majority of respondents rating it 4 or 5, suggesting that affordability is a key concern in their purchasing decisions. 
- **Single Portion:** the distribution is skewed towards 1 and 3. The demand for single-portion seafood products seems to vary, with some respondents being indifferent. 
- **Sustainable Sources:** many respondents rate sustainable sourcing highly (4 and 5), with the majority seeming to prioritize sustainability. 
- **Low Impact Environment:** similar to sustainable sourcing, most respondents rated low environmental impact positively (4 and 5), reflecting strong concern for environmental factors. 
- **Sustainable Packaging:** the majority of responses are positive, with most ratings clustered around 3 to 5, showing concern for sustainability in packaging. 
</span><span class="s0">#%% 
</span><span class="s1">sns.set(style=</span><span class="s3">&quot;whitegrid&quot;</span><span class="s1">)</span>
<span class="s1">df_melted = segmentation_data.melt(var_name=</span><span class="s3">'Variable'</span><span class="s1">, value_name=</span><span class="s3">'Value'</span><span class="s1">)</span>

<span class="s1">plt.figure(figsize=(</span><span class="s4">12</span><span class="s1">, </span><span class="s4">6</span><span class="s1">))</span>
<span class="s1">sns.boxplot(x=</span><span class="s3">'Variable'</span><span class="s1">, y=</span><span class="s3">'Value'</span><span class="s1">, data=df_melted, hue=</span><span class="s3">'Variable'</span><span class="s1">, palette=</span><span class="s3">'Set2'</span><span class="s1">, legend=</span><span class="s2">False</span><span class="s1">)</span>
<span class="s1">means = segmentation_data.mean()</span>

<span class="s2">for </span><span class="s1">i, mean </span><span class="s2">in </span><span class="s1">enumerate(means):</span>
    <span class="s1">plt.scatter(i, mean, color=</span><span class="s3">'black'</span><span class="s1">, label=</span><span class="s3">'Mean' </span><span class="s2">if </span><span class="s1">i == </span><span class="s4">0 </span><span class="s2">else </span><span class="s3">&quot;&quot;</span><span class="s1">, s=</span><span class="s4">100</span><span class="s1">, zorder=</span><span class="s4">10</span><span class="s1">)</span>
    
<span class="s1">plt.title(</span><span class="s3">'Boxplots of Each Variable'</span><span class="s1">, fontsize=</span><span class="s4">14</span><span class="s1">, fontweight=</span><span class="s3">'bold'</span><span class="s1">)</span>
<span class="s1">plt.xlabel(</span><span class="s3">''</span><span class="s1">)</span>
<span class="s1">plt.ylabel(</span><span class="s3">'Value'</span><span class="s1">)</span>
<span class="s1">plt.xticks(rotation=</span><span class="s4">45</span><span class="s1">)</span>
<span class="s1">plt.tight_layout()</span>
<span class="s1">plt.show()</span>
<span class="s0">#%% md 
</span><span class="s1">The boxplots provide a clear picture of how respondents perceive different attributes related to seafood products.  
 
Attributes like '*freshness*' and '*origin*' stand out with consistently high ratings, indicating they are **critical factors** for most consumers, as reflected by their tight distributions around 4 and 5.  
 
'*Sustainability*'-related aspects also show strong preferences, highlighting the importance of **environmental consciousness**.  
 
On the other hand, attributes such as '*cooking speed and simplicity*' exhibit wider variability, suggesting more diverse views on their significance. 
 
Notably, '*innovative products*' receive the lowest overall ratings, indicating **limited enthusiasm** for innovation in seafood.  
 
Overall, the data suggest a strong focus on traditional and sustainable qualities, with less emphasis on innovation or convenience. 
</span><span class="s0">#%% md 
</span><span class="s1">Last, we checked for **correlation** between attributes: 
</span><span class="s0">#%% 
</span><span class="s1">corr_matrix = segmentation_data.corr()</span>

<span class="s1">plt.figure(figsize=(</span><span class="s4">10</span><span class="s1">, </span><span class="s4">8</span><span class="s1">))</span>
<span class="s1">sns.heatmap(corr_matrix, annot=</span><span class="s2">True</span><span class="s1">, cmap=</span><span class="s3">'coolwarm'</span><span class="s1">, linewidths=</span><span class="s4">0.5</span><span class="s1">)</span>
<span class="s1">plt.title(</span><span class="s3">'Correlation matrix'</span><span class="s1">)</span>
<span class="s1">plt.show()</span>
<span class="s0">#%% md 
</span><span class="s1">-  Even though '*freshness*' and '*origin*' appear to be correlated (**0.69**), we decided to keep them both because they have significantly different implications in our decision and labour process. 
- '*Cooking_speed*' and '*cooking_simplicity*' have a high correlation (**0.78**). Our respondents may have perceived them as having the same meaning; therefore, we computed their mean and created a new variable named '*speed_n_simplicity*'. 
- In the correlation matrix, the last three variables related to sustainability are highly correlated. We first dropped '*low_impact_environment*' and retained '*sustainable_sources*' because of their similar implications. '*sustainable_packaging*' is also correlated with both, but we decided to keep it due to the increasing relevance given nowadays to this factor. 
</span><span class="s0">#%% 
</span><span class="s1">segmentation_data[</span><span class="s3">'speed_n_simplicity'</span><span class="s1">] = segmentation_data[[</span><span class="s3">'cooking_speed'</span><span class="s1">, </span><span class="s3">'cooking_simplicity'</span><span class="s1">]].mean(axis=</span><span class="s4">1</span><span class="s1">)</span>
<span class="s1">final_segmentation_data = segmentation_data.drop([</span><span class="s3">'cooking_speed'</span><span class="s1">, </span><span class="s3">'cooking_simplicity'</span><span class="s1">, </span><span class="s3">'low_impact_environment'</span><span class="s1">], axis=</span><span class="s4">1</span><span class="s1">)</span>
<span class="s1">final_segmentation_data.head()</span>
<span class="s0">#%% 
</span><span class="s1">corr_matrix = final_segmentation_data.corr()</span>

<span class="s1">plt.figure(figsize=(</span><span class="s4">10</span><span class="s1">, </span><span class="s4">8</span><span class="s1">))</span>
<span class="s1">sns.heatmap(corr_matrix, annot=</span><span class="s2">True</span><span class="s1">, cmap=</span><span class="s3">'coolwarm'</span><span class="s1">, linewidths=</span><span class="s4">0.5</span><span class="s1">)</span>
<span class="s1">plt.title(</span><span class="s3">'Correlation matrix'</span><span class="s1">)</span>
<span class="s1">plt.show()</span>
<span class="s0">#%% md 
</span><span class="s1">After correlation adjustment, there is a **visible improvement** in the heatmap.  
 
We can identify subgroups of variables with a medium correlation, which will be useful in the achievement of our goals (e.g. '*sustainable_sources*', '*origin*', '*freshness*', '*unprocessed*').   
</span><span class="s0">#%% md 
</span><span class="s1">--- 
 
 
### **2.2. Clustering** 
</span><span class="s0">#%% md 
</span><span class="s1">We continued the analysis by applying clustering techniques to the data, starting with K-Means. 
</span><span class="s0">#%% md 
</span><span class="s1">#### **2.2.1. Optimal number of clusters** 
</span><span class="s0">#%% md 
</span><span class="s1">At first, we computed the **optimal number of clusters** using the Elbow Method and the Silhouette Score. 
</span><span class="s0">#%% md 
</span><span class="s1">- **Elbow Method** 
</span><span class="s0">#%% 
</span><span class="s1">model = KMeans()</span>
<span class="s1">visualizer = KElbowVisualizer(model,timings=</span><span class="s2">False</span><span class="s1">, locate_elbow= </span><span class="s2">False</span><span class="s1">)</span>
<span class="s1">visualizer.fit(final_segmentation_data)</span>
<span class="s1">visualizer.show();</span>
<span class="s0">#%% md 
</span><span class="s1">- **Silhouette Method** 
</span><span class="s0">#%% 
</span><span class="s1">visualizer = KElbowVisualizer(model,metric=</span><span class="s3">'silhouette'</span><span class="s1">,timings=</span><span class="s2">False</span><span class="s1">, locate_elbow= </span><span class="s2">False</span><span class="s1">)</span>
<span class="s1">visualizer.fit(final_segmentation_data)</span>
<span class="s1">visualizer.show();</span>
<span class="s0">#%% md 
</span><span class="s1">Running multiple times the optimal number of clusters analysis, each time we found a lot of variability in results. This suggests a lack of stability in the clustering outcomes. The possible reasons could be: 
 
* **Weak cluster separation**. K-Means clustering is highly sensitive to the initialization of centroids, which can result in varying cluster assignments and metrics like the Silhouette Score and Distortion Score. This variability, coupled with weak cluster separations in the data (evidenced by the drop in the Silhouette Score after k=2), suggests that the clustering structure may not be well-defined. Consequently, K-Means might struggle to produce stable and meaningful clusters. 
* **Highly skewed distributions**.  Variables such as freshness, origin, nutritional_balance, and sustainable_sources are heavily skewed towards higher values (e.g., ratings of 4 or 5). This reduces variability in the data, causing clusters to overlap or making it difficult for K-Means to find distinct boundaries. 
 
 
Next, we performed clustering with K=4 and K=2 to understand if there are significant differences between clusters or the results above confirm our hypothesis. 
</span><span class="s0">#%% md 
</span><span class="s1">--- 
 
 
#### **2.2.2. K-Means (k=2)** 
</span><span class="s0">#%% 
# K-Means and assign clusters to data</span>
<span class="s1">kmeans2 = KMeans(n_clusters = </span><span class="s4">2</span><span class="s1">, random_state=</span><span class="s4">42</span><span class="s1">)</span>
<span class="s1">kmeans2.fit(final_segmentation_data)</span>
<span class="s1">final_segmentation_data_2 = final_segmentation_data.copy()</span>
<span class="s1">final_segmentation_data_2[</span><span class="s3">'Cluster_KMeans'</span><span class="s1">] = kmeans2.labels_ + </span><span class="s4">1</span>
<span class="s0">#%% 
# Silhouette Plot</span>
<span class="s1">cluster_labels = kmeans2.fit_predict(final_segmentation_data_2)</span>
<span class="s1">silhouette_vals = silhouette_samples(final_segmentation_data_2, cluster_labels)</span>
<span class="s1">silhouette_avg = silhouette_score(final_segmentation_data_2, cluster_labels)</span>
<span class="s1">n_clusters = </span><span class="s4">2</span>
<span class="s1">y_lower = </span><span class="s4">10</span>
<span class="s1">plt.figure(figsize=(</span><span class="s4">10</span><span class="s1">, </span><span class="s4">6</span><span class="s1">))</span>
<span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(n_clusters):</span>
    <span class="s1">ith_cluster_silhouette_values = silhouette_vals[cluster_labels == i]</span>
    <span class="s1">ith_cluster_silhouette_values.sort()</span>
    <span class="s1">size_cluster_i = ith_cluster_silhouette_values.shape[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">y_upper = y_lower + size_cluster_i</span>

    <span class="s1">plt.fill_betweenx(np.arange(y_lower, y_upper),</span>
                      <span class="s4">0</span><span class="s1">, ith_cluster_silhouette_values,</span>
                      <span class="s1">alpha=</span><span class="s4">0.7</span><span class="s1">)</span>
    <span class="s1">plt.text(-</span><span class="s4">0.05</span><span class="s1">, y_lower + </span><span class="s4">0.5 </span><span class="s1">* size_cluster_i, str(i))</span>
    <span class="s1">y_lower = y_upper + </span><span class="s4">10  </span><span class="s0"># 10 for spacing between clusters</span>

<span class="s1">plt.axvline(x=silhouette_avg, color=</span><span class="s3">&quot;red&quot;</span><span class="s1">, linestyle=</span><span class="s3">&quot;--&quot;</span><span class="s1">)</span>
<span class="s1">plt.title(</span><span class="s3">&quot;Silhouette Plot for KMeans Clustering&quot;</span><span class="s1">)</span>
<span class="s1">plt.xlabel(</span><span class="s3">&quot;Silhouette Coefficient&quot;</span><span class="s1">)</span>
<span class="s1">plt.ylabel(</span><span class="s3">&quot;Cluster&quot;</span><span class="s1">)</span>
<span class="s1">plt.show()</span>

<span class="s1">print(</span><span class="s3">&quot;Silhouette Score:&quot;</span><span class="s1">, silhouette_avg)</span>
<span class="s0">#%% 
# Compute Clusters' size</span>
<span class="s1">cluster_sizes = final_segmentation_data_2[</span><span class="s3">'Cluster_KMeans'</span><span class="s1">].value_counts()</span>
<span class="s1">cluster_percentages = (cluster_sizes / cluster_sizes.sum()) * </span><span class="s4">100</span>

<span class="s1">cluster_summary = pd.DataFrame({</span>
    <span class="s3">'Cluster'</span><span class="s1">: cluster_sizes.index,</span>
    <span class="s3">'Count'</span><span class="s1">: cluster_sizes.values,</span>
    <span class="s3">'Percentage'</span><span class="s1">: cluster_percentages.values</span>
<span class="s1">})</span>

<span class="s1">cluster_summary = cluster_summary.sort_values(by=</span><span class="s3">'Cluster'</span><span class="s1">).reset_index(drop=</span><span class="s2">True</span><span class="s1">)</span>
<span class="s1">cluster_summary</span>
<span class="s0">#%% 
# Compute centroids' means</span>
<span class="s1">centroids2 = final_segmentation_data_2.groupby(</span><span class="s3">&quot;Cluster_KMeans&quot;</span><span class="s1">).mean().T</span>
<span class="s1">centroids2.insert(</span><span class="s4">0</span><span class="s1">,</span><span class="s3">&quot;Population&quot;</span><span class="s1">,final_segmentation_data_2.mean())</span>
<span class="s1">centroids2.round(</span><span class="s4">2</span><span class="s1">)</span>
<span class="s0">#%% 
</span><span class="s1">X = final_segmentation_data_2.copy()</span>

<span class="s2">def </span><span class="s1">style_cols(col):</span>
    <span class="s2">if </span><span class="s1">col.name == </span><span class="s3">&quot;Population&quot;</span><span class="s1">: </span><span class="s2">return </span><span class="s1">[</span><span class="s3">&quot;background-color: white&quot;</span><span class="s1">]*len(col)</span>
    
    <span class="s1">segment = X[X.Cluster_KMeans == col.name]</span>
    <span class="s1">rest_pop = X[X.Cluster_KMeans != col.name]</span>
    
    <span class="s1">colors = []</span>
    <span class="s2">for </span><span class="s1">var_name, var_segment_mean </span><span class="s2">in </span><span class="s1">col.items():</span>
        <span class="s1">t_test, p_value = stats.ttest_ind(segment[var_name], rest_pop[var_name],equal_var=</span><span class="s2">False</span><span class="s1">)</span>
        <span class="s0">#_,p_value,_ = st.ttest_ind(segment[var_name],rest_pop[var_name],usevar=&quot;unequal&quot;)    </span>
        <span class="s2">if </span><span class="s1">p_value &lt; </span><span class="s4">0.05</span><span class="s1">:</span>
            <span class="s1">color = </span><span class="s3">&quot;background-color: green&quot; </span><span class="s2">if </span><span class="s1">var_segment_mean &gt; rest_pop[var_name].mean() </span><span class="s2">else </span><span class="s3">&quot;background-color: red&quot;</span>
        <span class="s2">elif </span><span class="s1">p_value &lt; </span><span class="s4">0.1</span><span class="s1">:</span>
            <span class="s1">color = </span><span class="s3">&quot;background-color: #90ee90&quot; </span><span class="s2">if </span><span class="s1">var_segment_mean &gt; rest_pop[var_name].mean() </span><span class="s2">else </span><span class="s3">&quot;background-color: #f1807e&quot;</span>
        <span class="s2">else</span><span class="s1">: color = </span><span class="s3">&quot;background-color: white&quot;</span>
        <span class="s1">colors.append(color)</span>
    <span class="s2">return </span><span class="s1">colors</span>

<span class="s1">centroids2.style.apply(style_cols).format(</span><span class="s3">&quot;{:.2f}&quot;</span><span class="s1">)</span>
<span class="s0">#%% md 
</span><span class="s1">Cluster 1 has consistently high values across all variables, while Cluster 2 shows consistently low values. This indicates a weak separation, with clusters primarily defined by overall rating levels, proving that clustering is inefficient. 
</span><span class="s0">#%% md 
</span><span class="s1">--- 
 
 
#### **2.2.3. K-Means (k=4)** 
</span><span class="s0">#%% 
# Compute KMeans and assign clusters to data</span>
<span class="s1">kmeans4 = KMeans(n_clusters = </span><span class="s4">4</span><span class="s1">, random_state=</span><span class="s4">42</span><span class="s1">)</span>
<span class="s1">kmeans4.fit(final_segmentation_data)</span>
<span class="s1">final_segmentation_data_4 = final_segmentation_data.copy()</span>
<span class="s1">final_segmentation_data_4[</span><span class="s3">'Cluster_KMeans'</span><span class="s1">] = kmeans4.labels_ +</span><span class="s4">1</span>
<span class="s1">final_segmentation_data_4</span>
<span class="s0">#%% 
# Silhouette Plot</span>
<span class="s1">cluster_labels = kmeans4.fit_predict(final_segmentation_data_4)</span>
<span class="s1">silhouette_vals = silhouette_samples(final_segmentation_data_4, cluster_labels)</span>
<span class="s1">silhouette_avg = silhouette_score(final_segmentation_data_4, cluster_labels)</span>
<span class="s1">n_clusters = </span><span class="s4">4</span>
<span class="s1">y_lower = </span><span class="s4">10</span>
<span class="s1">plt.figure(figsize=(</span><span class="s4">10</span><span class="s1">, </span><span class="s4">6</span><span class="s1">))</span>
<span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(n_clusters):</span>
    <span class="s1">ith_cluster_silhouette_values = silhouette_vals[cluster_labels == i]</span>
    <span class="s1">ith_cluster_silhouette_values.sort()</span>
    <span class="s1">size_cluster_i = ith_cluster_silhouette_values.shape[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">y_upper = y_lower + size_cluster_i</span>

    <span class="s1">plt.fill_betweenx(np.arange(y_lower, y_upper),</span>
                      <span class="s4">0</span><span class="s1">, ith_cluster_silhouette_values,</span>
                      <span class="s1">alpha=</span><span class="s4">0.7</span><span class="s1">)</span>
    <span class="s1">plt.text(-</span><span class="s4">0.05</span><span class="s1">, y_lower + </span><span class="s4">0.5 </span><span class="s1">* size_cluster_i, str(i))</span>
    <span class="s1">y_lower = y_upper + </span><span class="s4">10</span>

<span class="s1">plt.axvline(x=silhouette_avg, color=</span><span class="s3">&quot;red&quot;</span><span class="s1">, linestyle=</span><span class="s3">&quot;--&quot;</span><span class="s1">)</span>
<span class="s1">plt.title(</span><span class="s3">&quot;Silhouette Plot for KMeans Clustering&quot;</span><span class="s1">)</span>
<span class="s1">plt.xlabel(</span><span class="s3">&quot;Silhouette Coefficient&quot;</span><span class="s1">)</span>
<span class="s1">plt.ylabel(</span><span class="s3">&quot;Cluster&quot;</span><span class="s1">)</span>
<span class="s1">plt.show()</span>

<span class="s1">print(</span><span class="s3">&quot;Silhouette Score:&quot;</span><span class="s1">, silhouette_avg)</span>
<span class="s0">#%% 
# Compute Clusters' size</span>
<span class="s1">cluster_sizes = final_segmentation_data_4[</span><span class="s3">'Cluster_KMeans'</span><span class="s1">].value_counts()</span>
<span class="s1">cluster_percentages = (cluster_sizes / cluster_sizes.sum()) * </span><span class="s4">100</span>

<span class="s1">cluster_summary = pd.DataFrame({</span>
    <span class="s3">'Cluster'</span><span class="s1">: cluster_sizes.index,</span>
    <span class="s3">'Count'</span><span class="s1">: cluster_sizes.values,</span>
    <span class="s3">'Percentage'</span><span class="s1">: cluster_percentages.values</span>
<span class="s1">})</span>

<span class="s1">cluster_summary = cluster_summary.sort_values(by=</span><span class="s3">'Cluster'</span><span class="s1">).reset_index(drop=</span><span class="s2">True</span><span class="s1">)</span>
<span class="s1">cluster_summary</span>
<span class="s0">#%% 
# Compute centroids means</span>
<span class="s1">centroids4 = final_segmentation_data_4.groupby(</span><span class="s3">&quot;Cluster_KMeans&quot;</span><span class="s1">).mean().T</span>
<span class="s1">centroids4.insert(</span><span class="s4">0</span><span class="s1">,</span><span class="s3">&quot;Population&quot;</span><span class="s1">,final_segmentation_data_4.mean())</span>
<span class="s1">centroids4.round(</span><span class="s4">2</span><span class="s1">)</span>
<span class="s0">#%% 
# Compute means table</span>
<span class="s1">X = final_segmentation_data_4.copy()</span>
<span class="s1">centroids4.style.apply(style_cols).format(</span><span class="s3">&quot;{:.2f}&quot;</span><span class="s1">)</span>
<span class="s0">#%% md 
</span><span class="s1">Cluster 3 and Cluster 4 show consistently high values across most variables, while Cluster 1 and Cluster 2 show lower values, with Cluster 2 being the lowest overall. This pattern suggests a clear separation between high-value clusters (3 and 4) and low-value clusters (1 and 2), but the distinction within each group appears weaker. 
</span><span class="s0">#%% md 
</span><span class="s1">--- 
 
 
#### **2.2.4. Hierarchical Clustering** 
 
After performing K-means, we also tried the hierarchical approach. 
</span><span class="s0">#%% 
# Hierarchical Clustering</span>
<span class="s1">Z = linkage(final_segmentation_data, method=</span><span class="s3">'ward'</span><span class="s1">)</span>

<span class="s1">plt.figure(figsize=(</span><span class="s4">10</span><span class="s1">, </span><span class="s4">5</span><span class="s1">))</span>
<span class="s1">dendrogram(Z, leaf_rotation=</span><span class="s4">90</span><span class="s1">, leaf_font_size=</span><span class="s4">8</span><span class="s1">)</span>
<span class="s1">plt.title(</span><span class="s3">'Hierarchical Clustering Dendogram'</span><span class="s1">)</span>
<span class="s1">plt.xlabel(</span><span class="s3">'Observations'</span><span class="s1">)</span>
<span class="s1">plt.ylabel(</span><span class="s3">'Distance'</span><span class="s1">)</span>
<span class="s1">plt.show()</span>
<span class="s0">#%% md 
</span>
<span class="s1">Based on visual inspection, **four clusters** seem to be a suitable choice for cutting the dendrogram. To ensure the selection is optimal, we compute the silhouette scores for further validation. 
</span><span class="s0">#%% 
# Silhouette score</span>
<span class="s1">silhouette_scores = []</span>
<span class="s1">cluster_range = range(</span><span class="s4">2</span><span class="s1">, </span><span class="s4">10</span><span class="s1">) </span>

<span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">cluster_range:</span>
    <span class="s1">labels = fcluster(Z, k, criterion=</span><span class="s3">'maxclust'</span><span class="s1">)</span>
    <span class="s1">score = silhouette_score(final_segmentation_data, labels)</span>
    <span class="s1">silhouette_scores.append(score)</span>
    
<span class="s1">plt.figure(figsize=(</span><span class="s4">8</span><span class="s1">, </span><span class="s4">6</span><span class="s1">))</span>
<span class="s1">plt.plot(cluster_range, silhouette_scores, marker=</span><span class="s3">'o'</span><span class="s1">, linestyle=</span><span class="s3">'-'</span><span class="s1">, color=</span><span class="s3">'b'</span><span class="s1">)</span>
<span class="s1">plt.title(</span><span class="s3">'Silhouette Score vs Number of Clusters'</span><span class="s1">)</span>
<span class="s1">plt.xlabel(</span><span class="s3">'Number of Clusters (k)'</span><span class="s1">)</span>
<span class="s1">plt.ylabel(</span><span class="s3">'Silhouette Score'</span><span class="s1">)</span>
<span class="s1">plt.xticks(cluster_range)</span>
<span class="s1">plt.grid(</span><span class="s2">True</span><span class="s1">)</span>
<span class="s1">plt.show()</span>
<span class="s0">#%% md 
</span><span class="s1">Once again, the suggested number of clusters is 2, but we believe that this might be too few for our data. Therefore, 4 could still be the best solution in this case as well. 
We performed the same operations as we did for K-means but noticed that the characteristics of the clusters are almost identical. Therefore, for simplicity, we proceeded with the analysis of K-means results. 
</span><span class="s0">#%% md 
</span><span class="s1">--- 
 
 
### **2.3. PCA** 
</span><span class="s0">#%% md 
</span><span class="s1">Given the challenges encountered during our analysis and the poor separation of the clusters obtained, we decided to consider **Principal Component Analysis** to gain a better understanding of the data structure and improve the interpretability of our segmentation results. 
 
By applying PCA, we aim to identify the most relevant features that contribute to the differentiation of clusters and assess whether a lower-dimensional representation of the data can enhance the separation between them. 
</span><span class="s0">#%% md 
</span><span class="s1">Before proceeding with the PCA, we calculated the **Kaiser-Meyer-Olkin** (KMO), which is a measure of the sample's adequacy for factor analysis.  
</span><span class="s0">#%% 
</span><span class="s1">kmo_all, kmo_model = calculate_kmo(final_segmentation_data)</span>
<span class="s1">print(</span><span class="s3">f&quot;KMO Value: </span><span class="s5">{</span><span class="s1">kmo_model</span><span class="s5">}</span><span class="s3">&quot;</span><span class="s1">)</span>
<span class="s0">#%% md 
</span><span class="s1">A KMO value of 0.846 indicates that the data are **suitable for PCA**, supporting the decision to proceed with this technique for variable reduction. 
</span><span class="s0">#%% md 
</span><span class="s1">--- 
 
 
#### **2.3.1. Principal Components** 
</span><span class="s0">#%% 
# PCA</span>
<span class="s1">pca_full = PCA(n_components=len(final_segmentation_data.columns))</span>
<span class="s1">principal_components_full = pca_full.fit_transform(final_segmentation_data)</span>
<span class="s1">explained_variance = pca_full.explained_variance_ratio_</span>
<span class="s1">explained_variance_percent = explained_variance * </span><span class="s4">100 </span>
<span class="s1">cumulative_variance = explained_variance.cumsum()</span>

<span class="s1">plt.figure(figsize=(</span><span class="s4">8</span><span class="s1">,</span><span class="s4">4</span><span class="s1">))</span>
<span class="s1">plt.bar(range(</span><span class="s4">1</span><span class="s1">, len(explained_variance)+</span><span class="s4">1</span><span class="s1">), explained_variance, alpha=</span><span class="s4">0.7</span><span class="s1">)</span>
<span class="s1">plt.step(range(</span><span class="s4">1</span><span class="s1">, len(cumulative_variance)+</span><span class="s4">1</span><span class="s1">), cumulative_variance, where=</span><span class="s3">'mid'</span><span class="s1">, color=</span><span class="s3">'orange'</span><span class="s1">)</span>
<span class="s1">plt.ylabel(</span><span class="s3">'Explained Variance Ratio'</span><span class="s1">)</span>
<span class="s1">plt.xlabel(</span><span class="s3">'Principal Components'</span><span class="s1">)</span>
<span class="s1">plt.title(</span><span class="s3">'PCA Explained Variance'</span><span class="s1">)</span>
<span class="s1">plt.show()</span>
<span class="s0">#%% 
# PCA with 5 components</span>
<span class="s1">pca = PCA(n_components=</span><span class="s4">5</span><span class="s1">)</span>
<span class="s1">principal_components = pca.fit_transform(final_segmentation_data)</span>

<span class="s0"># Create a DataFrame with the 5 principal components</span>
<span class="s1">pca_df = pd.DataFrame(principal_components, </span>
                      <span class="s1">columns=[</span><span class="s3">'PC1'</span><span class="s1">,</span><span class="s3">'PC2'</span><span class="s1">,</span><span class="s3">'PC3'</span><span class="s1">,</span><span class="s3">'PC4'</span><span class="s1">,</span><span class="s3">'PC5'</span><span class="s1">])</span>

<span class="s0"># Explained variance ratios for the 5 PCs</span>
<span class="s1">explained_variance = pca.explained_variance_ratio_</span>
<span class="s1">cumulative_variance = explained_variance.cumsum()</span>


<span class="s0"># Plot explained variance</span>
<span class="s1">plt.figure(figsize=(</span><span class="s4">8</span><span class="s1">,</span><span class="s4">4</span><span class="s1">))</span>
<span class="s1">plt.bar(range(</span><span class="s4">1</span><span class="s1">, len(explained_variance)+</span><span class="s4">1</span><span class="s1">), explained_variance, alpha=</span><span class="s4">0.7</span><span class="s1">)</span>
<span class="s1">plt.step(range(</span><span class="s4">1</span><span class="s1">, len(cumulative_variance)+</span><span class="s4">1</span><span class="s1">), cumulative_variance, </span>
         <span class="s1">where=</span><span class="s3">'mid'</span><span class="s1">, color=</span><span class="s3">'orange'</span><span class="s1">)</span>
<span class="s1">plt.ylabel(</span><span class="s3">'Explained Variance Ratio'</span><span class="s1">)</span>
<span class="s1">plt.xlabel(</span><span class="s3">'Principal Components'</span><span class="s1">)</span>
<span class="s1">plt.title(</span><span class="s3">'PCA Explained Variance (5 Components)'</span><span class="s1">)</span>
<span class="s1">plt.show()</span>

<span class="s1">print(</span><span class="s3">&quot;Explained variance per component:&quot;</span><span class="s1">, explained_variance)</span>
<span class="s1">print(</span><span class="s3">&quot;Cumulative explained variance:&quot;</span><span class="s1">, cumulative_variance)</span>
<span class="s0">#%% md 
</span><span class="s1">Using 5 principal components, the cumulative explained variance is approximately **81.45%**. 
</span><span class="s0">#%% 
</span><span class="s1">loadings = pd.DataFrame(</span>
    <span class="s1">pca.components_.T, </span>
    <span class="s1">columns=[</span><span class="s3">f'PC</span><span class="s5">{</span><span class="s1">i+</span><span class="s4">1</span><span class="s5">}</span><span class="s3">' </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(pca.n_components_)],</span>
    <span class="s1">index=final_segmentation_data.columns</span>
<span class="s1">)</span>

<span class="s1">print(</span><span class="s3">&quot;PCA Loadings (Coefficients):&quot;</span><span class="s1">)</span>
<span class="s1">print(loadings)</span>

<span class="s2">for </span><span class="s1">pc </span><span class="s2">in </span><span class="s1">loadings.columns:</span>
    <span class="s0"># Sort by absolute value to find highest magnitude (positive or negative)</span>
    <span class="s1">sorted_loadings = loadings[pc].abs().sort_values(ascending=</span><span class="s2">False</span><span class="s1">)</span>
    <span class="s1">top_features = sorted_loadings.head(</span><span class="s4">5</span><span class="s1">) </span>
    <span class="s1">print(</span><span class="s3">f&quot;</span><span class="s5">\n</span><span class="s3">Top loadings for </span><span class="s5">{</span><span class="s1">pc</span><span class="s5">}</span><span class="s3">:&quot;</span><span class="s1">)</span>
    <span class="s1">print(top_features)</span>
<span class="s0">#%% md 
</span><span class="s1">- **PC1: “Origin Awareness and Sustainability”**: represents a strong focus on sustainability and product origin. 
Individuals with high scores on this component prefer products with certified origin, sustainably packaged, and sourced from environmentally responsible suppliers. 
 
 
- **PC2: “Practicality”**: highlights a strong orientation toward practicality. 
Individuals with high scores on this component prefer products that are quick and easy to prepare, often in single-portion formats. 
 
 
- **PC3: “Price and Sustainability”**: combines sustainability, cost, and quality. 
Individuals with high scores on this component seek a balance between price, freshness, and sustainable packaging, indicating a responsible yet economically conscious consumption. 
 
 
- **PC4: “Innovation and Simplicity”**: reflects an interest in innovative products that simplify life. 
Individuals with high scores on this component appreciate modern and technological solutions that make product use or consumption faster and more convenient. 
 
 
- **PC5: “Nutritional Control”**: emphasizes the importance of health and dietary control. 
Individuals with high scores on this component prefer products that offer optimal nutritional balance and well-defined portions, reflecting a focus on diet management. 
</span><span class="s0">#%% md 
</span><span class="s1">--- 
 
 
#### **2.3.2. Optimal number of clusters** 
</span><span class="s0">#%% md 
</span><span class="s1">- **Elbow Method** 
</span><span class="s0">#%% 
</span><span class="s1">inertias = []</span>
<span class="s1">K_range = range(</span><span class="s4">1</span><span class="s1">, </span><span class="s4">11</span><span class="s1">)</span>

<span class="s2">for </span><span class="s1">k </span><span class="s2">in </span><span class="s1">K_range:</span>
    <span class="s1">kmeans = KMeans(n_clusters=k, random_state=</span><span class="s4">42</span><span class="s1">)</span>
    <span class="s1">kmeans.fit(pca_df)</span>
    <span class="s1">inertias.append(kmeans.inertia_)</span>

<span class="s1">plt.figure(figsize=(</span><span class="s4">8</span><span class="s1">,</span><span class="s4">5</span><span class="s1">))</span>
<span class="s1">plt.plot(K_range, inertias, </span><span class="s3">'bo-'</span><span class="s1">)</span>
<span class="s1">plt.title(</span><span class="s3">&quot;Elbow Method for K-Means&quot;</span><span class="s1">)</span>
<span class="s1">plt.xlabel(</span><span class="s3">&quot;Number of clusters (k)&quot;</span><span class="s1">)</span>
<span class="s1">plt.ylabel(</span><span class="s3">&quot;Inertia (Sum of Squared Distances)&quot;</span><span class="s1">)</span>
<span class="s1">plt.show()</span>
<span class="s0">#%% md 
</span><span class="s1">Based on the elbow method, K=4 or K=2 could be a reasonable choice, as the inertia decreases at a slower rate beyond these points. 
</span><span class="s0">#%% md 
</span><span class="s1">--- 
 
 
#### **2.3.3. K-Means (k=4)** 
</span><span class="s0">#%% 
</span><span class="s2">if </span><span class="s3">'Cluster' </span><span class="s2">in </span><span class="s1">pca_df.columns:</span>
    <span class="s1">pca_df.drop(</span><span class="s3">'Cluster'</span><span class="s1">, axis=</span><span class="s4">1</span><span class="s1">, inplace=</span><span class="s2">True</span><span class="s1">)</span>
    
<span class="s0"># Fit K-Means</span>
<span class="s1">k=</span><span class="s4">4</span>
<span class="s1">kmeans = KMeans(n_clusters=k, random_state=</span><span class="s4">42</span><span class="s1">)</span>
<span class="s1">kmeans.fit(pca_df[[</span><span class="s3">'PC1'</span><span class="s1">,</span><span class="s3">'PC2'</span><span class="s1">, </span><span class="s3">'PC3'</span><span class="s1">, </span><span class="s3">'PC4'</span><span class="s1">, </span><span class="s3">'PC5'</span><span class="s1">]]) </span>

<span class="s1">pca_df[</span><span class="s3">'Cluster'</span><span class="s1">] = kmeans.labels_ + </span><span class="s4">1</span>

<span class="s0"># Plot each cluster with a different color</span>
<span class="s1">colors = [</span><span class="s3">'red'</span><span class="s1">, </span><span class="s3">'green'</span><span class="s1">, </span><span class="s3">'blue'</span><span class="s1">, </span><span class="s3">'magenta'</span><span class="s1">, </span><span class="s3">'orange'</span><span class="s1">]</span>
<span class="s1">plt.figure(figsize=(</span><span class="s4">8</span><span class="s1">,</span><span class="s4">6</span><span class="s1">))</span>

<span class="s2">for </span><span class="s1">cluster_label </span><span class="s2">in </span><span class="s1">range(</span><span class="s4">1</span><span class="s1">, k + </span><span class="s4">1</span><span class="s1">):</span>
    <span class="s1">subset = pca_df[pca_df[</span><span class="s3">'Cluster'</span><span class="s1">] == cluster_label]</span>
    <span class="s1">plt.scatter(</span>
        <span class="s1">subset[</span><span class="s3">'PC1'</span><span class="s1">], subset[</span><span class="s3">'PC2'</span><span class="s1">], </span>
        <span class="s1">s=</span><span class="s4">100</span><span class="s1">, </span>
        <span class="s1">c=colors[cluster_label - </span><span class="s4">1</span><span class="s1">], </span>
        <span class="s1">label=</span><span class="s3">f'Cluster </span><span class="s5">{</span><span class="s1">cluster_label</span><span class="s5">}</span><span class="s3">'</span>
    <span class="s1">)</span>

<span class="s1">plt.xlabel(</span><span class="s3">'Principal Component 1'</span><span class="s1">)</span>
<span class="s1">plt.ylabel(</span><span class="s3">'Principal Component 2'</span><span class="s1">)</span>
<span class="s1">plt.title(</span><span class="s3">'K-Means Clusters (PCA-Reduced Data)'</span><span class="s1">)</span>
<span class="s1">plt.legend()</span>
<span class="s1">plt.show()</span>
<span class="s0">#%% 
</span><span class="s1">score = silhouette_score(pca_df[[</span><span class="s3">'PC1'</span><span class="s1">,</span><span class="s3">'PC2'</span><span class="s1">,</span><span class="s3">'PC3'</span><span class="s1">,</span><span class="s3">'PC4'</span><span class="s1">,</span><span class="s3">'PC5'</span><span class="s1">]], pca_df[</span><span class="s3">'Cluster'</span><span class="s1">])</span>
<span class="s1">print(</span><span class="s3">&quot;Silhouette Score:&quot;</span><span class="s1">, score)</span>
<span class="s0">#%% 
</span><span class="s1">final_segmentation_data[</span><span class="s3">'Cluster'</span><span class="s1">] = pca_df[</span><span class="s3">'Cluster'</span><span class="s1">]</span>

<span class="s0"># Compute mean of original variables by cluster</span>
<span class="s1">cluster_means = final_segmentation_data.groupby(</span><span class="s3">'Cluster'</span><span class="s1">).mean()</span>
<span class="s1">print(</span><span class="s3">&quot;Cluster Means:&quot;</span><span class="s1">)</span>
<span class="s1">print(cluster_means)</span>
<span class="s0">#%% md 
</span><span class="s1">- **Cluster 1: “Conscious Consumers”**. The members of this group particularly value freshness, origin, and sustainability, while also giving importance to price. They do not prioritize innovative products but pay attention to overall quality and nutritional balance. 
 
- **Cluster 2: “Balanced Buyers”**. The members of this cluster exhibit a balanced behavior, assigning importance across various dimensions: freshness, origin, nutritional balance, sustainability, and price. 
 
- **Cluster 3: “Eco-Engaged Innovators”**. The members of this cluster stand out for their high attention to sustainability (both sources and packaging) and greatly appreciate unprocessed foods. However, they do not overlook freshness and product origin. 
 
- **Cluster 4: “Traditional Simplicity Seekers”**. The members of this cluster take a more traditional approach, with less interest in innovation and sustainability. Nevertheless, they value freshness, origin, and simplicity in their choices. 
</span><span class="s0">#%% md 
</span><span class="s1">--- 
 
 
#### **2.3.4. Clusters Analysis (k=2)** 
</span><span class="s0">#%% 
</span><span class="s2">if </span><span class="s3">'Cluster' </span><span class="s2">in </span><span class="s1">pca_df.columns:</span>
    <span class="s1">pca_df.drop(</span><span class="s3">'Cluster'</span><span class="s1">, axis=</span><span class="s4">1</span><span class="s1">, inplace=</span><span class="s2">True</span><span class="s1">)</span>
    
<span class="s0"># Fit K-Means</span>
<span class="s1">k=</span><span class="s4">2</span>
<span class="s1">kmeans = KMeans(n_clusters=k, random_state=</span><span class="s4">42</span><span class="s1">)</span>
<span class="s1">kmeans.fit(pca_df[[</span><span class="s3">'PC1'</span><span class="s1">,</span><span class="s3">'PC2'</span><span class="s1">, </span><span class="s3">'PC3'</span><span class="s1">, </span><span class="s3">'PC4'</span><span class="s1">, </span><span class="s3">'PC5'</span><span class="s1">]])</span>

<span class="s1">pca_df[</span><span class="s3">'Cluster'</span><span class="s1">] = kmeans.labels_ + </span><span class="s4">1</span>

<span class="s0"># Plot each cluster with a different color</span>
<span class="s1">colors = [</span><span class="s3">'red'</span><span class="s1">, </span><span class="s3">'green'</span><span class="s1">]</span>
<span class="s1">plt.figure(figsize=(</span><span class="s4">8</span><span class="s1">,</span><span class="s4">6</span><span class="s1">))</span>

<span class="s2">for </span><span class="s1">cluster_label </span><span class="s2">in </span><span class="s1">range(</span><span class="s4">1</span><span class="s1">, k + </span><span class="s4">1</span><span class="s1">):</span>
    <span class="s1">subset = pca_df[pca_df[</span><span class="s3">'Cluster'</span><span class="s1">] == cluster_label]</span>
    <span class="s1">plt.scatter(</span>
        <span class="s1">subset[</span><span class="s3">'PC1'</span><span class="s1">], subset[</span><span class="s3">'PC2'</span><span class="s1">], </span>
        <span class="s1">s=</span><span class="s4">100</span><span class="s1">, </span>
        <span class="s1">c=colors[cluster_label - </span><span class="s4">1</span><span class="s1">], </span>
        <span class="s1">label=</span><span class="s3">f'Cluster </span><span class="s5">{</span><span class="s1">cluster_label</span><span class="s5">}</span><span class="s3">'</span>
    <span class="s1">)</span>

<span class="s1">plt.xlabel(</span><span class="s3">'Principal Component 1'</span><span class="s1">)</span>
<span class="s1">plt.ylabel(</span><span class="s3">'Principal Component 2'</span><span class="s1">)</span>
<span class="s1">plt.title(</span><span class="s3">'K-Means Clusters (PCA-Reduced Data)'</span><span class="s1">)</span>
<span class="s1">plt.legend()</span>
<span class="s1">plt.show()</span>
<span class="s0">#%% 
</span><span class="s1">score = silhouette_score(pca_df[[</span><span class="s3">'PC1'</span><span class="s1">,</span><span class="s3">'PC2'</span><span class="s1">,</span><span class="s3">'PC3'</span><span class="s1">,</span><span class="s3">'PC4'</span><span class="s1">,</span><span class="s3">'PC5'</span><span class="s1">]], pca_df[</span><span class="s3">'Cluster'</span><span class="s1">])</span>
<span class="s1">print(</span><span class="s3">&quot;Silhouette Score:&quot;</span><span class="s1">, score)</span>
<span class="s0">#%% 
</span><span class="s1">final_segmentation_data[</span><span class="s3">'Cluster'</span><span class="s1">] = pca_df[</span><span class="s3">'Cluster'</span><span class="s1">]</span>

<span class="s0"># Compute mean of original variables by cluster</span>
<span class="s1">cluster_means = final_segmentation_data.groupby(</span><span class="s3">'Cluster'</span><span class="s1">).mean()</span>
<span class="s1">print(</span><span class="s3">&quot;Cluster Means:&quot;</span><span class="s1">)</span>
<span class="s1">print(cluster_means)</span>
<span class="s0">#%% md 
</span><span class="s1">- **Cluster 1: “Sustainability Enthusiasts”**. Individuals in this cluster place great importance on sustainability (both sources and packaging), freshness, and the origin of products, while also giving significant weight to price. They also appreciate unprocessed foods and a good nutritional balance. However, they show little interest in innovative products and single portions. 
 
- **Cluster 2: “Traditional Value Seekers”**. Individuals in this cluster take a more traditional approach, giving less importance to packaging sustainability and freshness compared to Cluster 1. They show very little interest in innovative products but still value nutritional balance, product origin, and the sustainability of sources. 
</span><span class="s0">#%% md 
</span><span class="s1">Comparing the results obtained with PCA to those obtained previously, we did not observe a significant improvement in cluster separation. Although PCA allowed us to reduce the dimensionality of the data and visualize the clusters in a lower-dimensional space, the overall structure remained similar to the original analysis. The silhouette scores did not show a substantial increase, indicating that the application of PCA did not lead to more distinct or well-defined clusters. 
</span><span class="s0">#%% md 
</span><span class="s1">--- 
 
 
### **2.4. Variable selection** 
</span><span class="s0">#%% md 
</span><span class="s1">Since the results obtained so far have not been satisfactory, we conducted further analyses by selecting a reduced set of variables for clustering, excluding those that showed significant skewness. However, at this stage, we decided to retain the price variable despite its asymmetric distribution, as it could capture meaningful patterns and underlying trends that could be relevant to the analysis. 
</span><span class="s0">#%% 
</span><span class="s1">reduced_data = final_segmentation_data[[</span><span class="s3">'innovative_products'</span><span class="s1">, </span><span class="s3">'price'</span><span class="s1">, </span><span class="s3">'single_portion'</span><span class="s1">, </span><span class="s3">'speed_n_simplicity'</span><span class="s1">]]</span>
<span class="s1">reduced_data.head()</span>
<span class="s0">#%% 
</span><span class="s1">num_variables = len(reduced_data.columns) </span>
<span class="s1">num_rows = num_variables // </span><span class="s4">3 </span><span class="s1">+ (num_variables % </span><span class="s4">3 </span><span class="s1">&gt; </span><span class="s4">0</span><span class="s1">)</span>

<span class="s1">fig, axes = plt.subplots(nrows=num_rows, ncols=</span><span class="s4">3</span><span class="s1">, figsize=(</span><span class="s4">15</span><span class="s1">, num_rows * </span><span class="s4">4</span><span class="s1">), tight_layout=</span><span class="s2">True</span><span class="s1">)</span>
<span class="s1">axes = axes.flatten()</span>

<span class="s2">for </span><span class="s1">index, column </span><span class="s2">in </span><span class="s1">enumerate(reduced_data.columns):</span>
    <span class="s1">axes[index].hist(reduced_data[column], bins=</span><span class="s4">5</span><span class="s1">, range=(</span><span class="s4">1</span><span class="s1">, </span><span class="s4">5</span><span class="s1">), alpha=</span><span class="s4">0.7</span><span class="s1">, color=</span><span class="s3">'blue'</span><span class="s1">, edgecolor=</span><span class="s3">'black'</span><span class="s1">)</span>
    <span class="s1">axes[index].set_title(column)</span>
    <span class="s1">axes[index].set_xlabel(</span><span class="s3">'Rating'</span><span class="s1">)</span>
    <span class="s1">axes[index].set_ylabel(</span><span class="s3">'Frequency'</span><span class="s1">)</span>
    <span class="s1">axes[index].set_xticks(range(</span><span class="s4">1</span><span class="s1">, </span><span class="s4">6</span><span class="s1">))</span>

<span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(index + </span><span class="s4">1</span><span class="s1">, len(axes)):</span>
    <span class="s1">axes[i].axis(</span><span class="s3">'off'</span><span class="s1">)</span>

<span class="s1">plt.show()</span>
<span class="s0">#%% md 
</span><span class="s1">--- 
 
 
#### **2.4.1. Optimal number of clusters**. 
</span><span class="s0">#%% md 
</span><span class="s1">- **Elbow method** 
</span><span class="s0">#%% 
</span><span class="s1">model = KMeans()</span>
<span class="s1">visualizer = KElbowVisualizer(model,timings=</span><span class="s2">False</span><span class="s1">, locate_elbow= </span><span class="s2">False</span><span class="s1">)</span>
<span class="s1">visualizer.fit(reduced_data)</span>
<span class="s1">visualizer.show();</span>
<span class="s0">#%% md 
</span><span class="s1">- **Silhouette method** 
</span><span class="s0">#%% 
</span><span class="s1">visualizer = KElbowVisualizer(model,metric=</span><span class="s3">'silhouette'</span><span class="s1">,timings=</span><span class="s2">False</span><span class="s1">, locate_elbow= </span><span class="s2">False</span><span class="s1">)</span>
<span class="s1">visualizer.fit(reduced_data)</span>
<span class="s1">visualizer.show();</span>
<span class="s0">#%% md 
</span><span class="s1">Based on the plots from both the elbow and silhouette methods, **no optimal number of clusters** clearly emerges, as each method produces inconsistent results with every run.  
 
This suggests that the data lacks stable and well-separated clusters. The silhouette plot indicates that K=2 yields the highest score, whereas in the elbow plot the inflection point seems to be at K=4, so we decided to apply K-Means clustering with both values to further assess their effectiveness. 
</span><span class="s0">#%% md 
</span><span class="s1">--- 
 
 
#### **2.4.2. K-Means (k=2)** 
</span><span class="s0">#%% 
# K-Means and assign clusters to data</span>
<span class="s1">reduced_data = reduced_data.copy()</span>
<span class="s1">kmeans2 = KMeans(n_clusters = </span><span class="s4">2</span><span class="s1">, random_state=</span><span class="s4">42</span><span class="s1">)</span>
<span class="s1">kmeans2.fit(reduced_data)</span>
<span class="s1">reduced_data[</span><span class="s3">'Cluster_K2'</span><span class="s1">] = kmeans2.labels_ + </span><span class="s4">1</span>
<span class="s0">#%% 
# Silhouette Plot</span>
<span class="s1">cluster_labels = kmeans2.fit_predict(reduced_data)</span>
<span class="s1">silhouette_vals = silhouette_samples(reduced_data, cluster_labels)</span>
<span class="s1">silhouette_avg = silhouette_score(reduced_data, cluster_labels)</span>
<span class="s1">n_clusters = </span><span class="s4">2</span>
<span class="s1">y_lower = </span><span class="s4">10</span>
<span class="s1">plt.figure(figsize=(</span><span class="s4">10</span><span class="s1">, </span><span class="s4">6</span><span class="s1">))</span>
<span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(n_clusters):</span>
    <span class="s1">ith_cluster_silhouette_values = silhouette_vals[cluster_labels == i]</span>
    <span class="s1">ith_cluster_silhouette_values.sort()</span>
    <span class="s1">size_cluster_i = ith_cluster_silhouette_values.shape[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">y_upper = y_lower + size_cluster_i</span>

    <span class="s1">plt.fill_betweenx(np.arange(y_lower, y_upper),</span>
                      <span class="s4">0</span><span class="s1">, ith_cluster_silhouette_values,</span>
                      <span class="s1">alpha=</span><span class="s4">0.7</span><span class="s1">)</span>
    <span class="s1">plt.text(-</span><span class="s4">0.05</span><span class="s1">, y_lower + </span><span class="s4">0.5 </span><span class="s1">* size_cluster_i, str(i))</span>
    <span class="s1">y_lower = y_upper + </span><span class="s4">10  </span><span class="s0"># 10 for spacing between clusters</span>

<span class="s1">plt.axvline(x=silhouette_avg, color=</span><span class="s3">&quot;red&quot;</span><span class="s1">, linestyle=</span><span class="s3">&quot;--&quot;</span><span class="s1">)</span>
<span class="s1">plt.title(</span><span class="s3">&quot;Silhouette Plot for KMeans Clustering&quot;</span><span class="s1">)</span>
<span class="s1">plt.xlabel(</span><span class="s3">&quot;Silhouette Coefficient&quot;</span><span class="s1">)</span>
<span class="s1">plt.ylabel(</span><span class="s3">&quot;Cluster&quot;</span><span class="s1">)</span>
<span class="s1">plt.show()</span>

<span class="s1">print(</span><span class="s3">&quot;Silhouette Score:&quot;</span><span class="s1">, silhouette_avg)</span>
<span class="s0">#%% md 
</span><span class="s1">#### **2.4.3. K-Means (k=4)** 
</span><span class="s0">#%% 
# Compute KMeans and assign clusters to data</span>
<span class="s1">reduced_data.drop([</span><span class="s3">'Cluster_K2'</span><span class="s1">], axis=</span><span class="s4">1</span><span class="s1">, inplace=</span><span class="s2">True</span><span class="s1">)</span>
<span class="s1">kmeans4 = KMeans(n_clusters = </span><span class="s4">4</span><span class="s1">, random_state=</span><span class="s4">42</span><span class="s1">)</span>
<span class="s1">kmeans4.fit(reduced_data)</span>
<span class="s1">reduced_data[</span><span class="s3">'Cluster_K4'</span><span class="s1">] = kmeans4.labels_ + </span><span class="s4">1</span>
<span class="s0">#%% 
# Silhouette Plot</span>
<span class="s1">cluster_labels = kmeans4.fit_predict(reduced_data)</span>
<span class="s1">silhouette_vals = silhouette_samples(reduced_data, cluster_labels)</span>
<span class="s1">silhouette_avg = silhouette_score(reduced_data, cluster_labels)</span>
<span class="s1">n_clusters = </span><span class="s4">4</span>
<span class="s1">y_lower = </span><span class="s4">10</span>
<span class="s1">plt.figure(figsize=(</span><span class="s4">10</span><span class="s1">, </span><span class="s4">6</span><span class="s1">))</span>
<span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(n_clusters):</span>
    <span class="s1">ith_cluster_silhouette_values = silhouette_vals[cluster_labels == i]</span>
    <span class="s1">ith_cluster_silhouette_values.sort()</span>
    <span class="s1">size_cluster_i = ith_cluster_silhouette_values.shape[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">y_upper = y_lower + size_cluster_i</span>

    <span class="s1">plt.fill_betweenx(np.arange(y_lower, y_upper),</span>
                      <span class="s4">0</span><span class="s1">, ith_cluster_silhouette_values,</span>
                      <span class="s1">alpha=</span><span class="s4">0.7</span><span class="s1">)</span>
    <span class="s1">plt.text(-</span><span class="s4">0.05</span><span class="s1">, y_lower + </span><span class="s4">0.5 </span><span class="s1">* size_cluster_i, str(i))</span>
    <span class="s1">y_lower = y_upper + </span><span class="s4">10  </span><span class="s0"># 10 for spacing between clusters</span>

<span class="s1">plt.axvline(x=silhouette_avg, color=</span><span class="s3">&quot;red&quot;</span><span class="s1">, linestyle=</span><span class="s3">&quot;--&quot;</span><span class="s1">)</span>
<span class="s1">plt.title(</span><span class="s3">&quot;Silhouette Plot for KMeans Clustering&quot;</span><span class="s1">)</span>
<span class="s1">plt.xlabel(</span><span class="s3">&quot;Silhouette Coefficient&quot;</span><span class="s1">)</span>
<span class="s1">plt.ylabel(</span><span class="s3">&quot;Cluster&quot;</span><span class="s1">)</span>
<span class="s1">plt.show()</span>

<span class="s1">print(</span><span class="s3">&quot;Silhouette Score:&quot;</span><span class="s1">, silhouette_avg)</span>
<span class="s0">#%% md 
</span><span class="s1">Considering only these variables, there is a slight improvement in the silhouette score values, but they remain low. Therefore, we decided to also exclude the price variable. 
</span><span class="s0">#%% 
</span><span class="s1">reduced_data.drop([</span><span class="s3">'Cluster_K4'</span><span class="s1">], axis=</span><span class="s4">1</span><span class="s1">, inplace=</span><span class="s2">True</span><span class="s1">)</span>
<span class="s1">reduced_data2 =  final_segmentation_data[[</span><span class="s3">'innovative_products'</span><span class="s1">, </span><span class="s3">'single_portion'</span><span class="s1">, </span><span class="s3">'speed_n_simplicity'</span><span class="s1">]]</span>
<span class="s1">reduced_data2.head()</span>
<span class="s0">#%% md 
</span><span class="s1">- **Elbow method** 
</span><span class="s0">#%% 
</span><span class="s1">model = KMeans()</span>
<span class="s1">visualizer = KElbowVisualizer(model,timings=</span><span class="s2">False</span><span class="s1">, locate_elbow= </span><span class="s2">False</span><span class="s1">)</span>
<span class="s1">visualizer.fit(reduced_data2)</span>
<span class="s1">visualizer.show();</span>
<span class="s0">#%% md 
</span><span class="s1">- **Silhouette method** 
</span><span class="s0">#%% 
</span><span class="s1">visualizer = KElbowVisualizer(model,metric=</span><span class="s3">'silhouette'</span><span class="s1">,timings=</span><span class="s2">False</span><span class="s1">, locate_elbow= </span><span class="s2">False</span><span class="s1">)</span>
<span class="s1">visualizer.fit(reduced_data2)</span>
<span class="s1">visualizer.show();</span>
<span class="s0">#%% md 
</span><span class="s1">Once again, an optimal number of clusters does not clearly emerge, and the results change when rerunning the plots, indicating that the clusters are not well separated. 
</span><span class="s0">#%% 
# K-Means and assign clusters to data</span>
<span class="s1">reduced_data2 = reduced_data2.copy()</span>
<span class="s1">kmeans2 = KMeans(n_clusters = </span><span class="s4">2</span><span class="s1">, random_state=</span><span class="s4">42</span><span class="s1">)</span>
<span class="s1">kmeans2.fit(reduced_data2)</span>
<span class="s1">reduced_data2[</span><span class="s3">'Cluster_K2'</span><span class="s1">] = kmeans2.labels_ + </span><span class="s4">1</span>
<span class="s0">#%% 
# Silhouette Plot</span>
<span class="s1">cluster_labels = kmeans2.fit_predict(reduced_data2)</span>
<span class="s1">silhouette_vals = silhouette_samples(reduced_data2, cluster_labels)</span>
<span class="s1">silhouette_avg = silhouette_score(reduced_data2, cluster_labels)</span>
<span class="s1">n_clusters = </span><span class="s4">2</span>
<span class="s1">y_lower = </span><span class="s4">10</span>
<span class="s1">plt.figure(figsize=(</span><span class="s4">10</span><span class="s1">, </span><span class="s4">6</span><span class="s1">))</span>
<span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(n_clusters):</span>
    <span class="s1">ith_cluster_silhouette_values = silhouette_vals[cluster_labels == i]</span>
    <span class="s1">ith_cluster_silhouette_values.sort()</span>
    <span class="s1">size_cluster_i = ith_cluster_silhouette_values.shape[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">y_upper = y_lower + size_cluster_i</span>

    <span class="s1">plt.fill_betweenx(np.arange(y_lower, y_upper),</span>
                      <span class="s4">0</span><span class="s1">, ith_cluster_silhouette_values,</span>
                      <span class="s1">alpha=</span><span class="s4">0.7</span><span class="s1">)</span>
    <span class="s1">plt.text(-</span><span class="s4">0.05</span><span class="s1">, y_lower + </span><span class="s4">0.5 </span><span class="s1">* size_cluster_i, str(i))</span>
    <span class="s1">y_lower = y_upper + </span><span class="s4">10  </span><span class="s0"># 10 for spacing between clusters</span>

<span class="s1">plt.axvline(x=silhouette_avg, color=</span><span class="s3">&quot;red&quot;</span><span class="s1">, linestyle=</span><span class="s3">&quot;--&quot;</span><span class="s1">)</span>
<span class="s1">plt.title(</span><span class="s3">&quot;Silhouette Plot for KMeans Clustering&quot;</span><span class="s1">)</span>
<span class="s1">plt.xlabel(</span><span class="s3">&quot;Silhouette Coefficient&quot;</span><span class="s1">)</span>
<span class="s1">plt.ylabel(</span><span class="s3">&quot;Cluster&quot;</span><span class="s1">)</span>
<span class="s1">plt.show()</span>

<span class="s1">print(</span><span class="s3">&quot;Silhouette Score:&quot;</span><span class="s1">, silhouette_avg)</span>
<span class="s0">#%% 
# Compute KMeans and assign clusters to data</span>
<span class="s1">reduced_data2.drop([</span><span class="s3">'Cluster_K2'</span><span class="s1">], axis=</span><span class="s4">1</span><span class="s1">, inplace=</span><span class="s2">True</span><span class="s1">)</span>
<span class="s1">kmeans4 = KMeans(n_clusters = </span><span class="s4">4</span><span class="s1">, random_state=</span><span class="s4">42</span><span class="s1">)</span>
<span class="s1">kmeans4.fit(reduced_data2)</span>
<span class="s1">reduced_data2[</span><span class="s3">'Cluster_K4'</span><span class="s1">] = kmeans4.labels_ + </span><span class="s4">1</span>
<span class="s0">#%% 
# Silhouette Plot</span>
<span class="s1">cluster_labels = kmeans4.fit_predict(reduced_data2)</span>
<span class="s1">silhouette_vals = silhouette_samples(reduced_data2, cluster_labels)</span>
<span class="s1">silhouette_avg = silhouette_score(reduced_data2, cluster_labels)</span>
<span class="s1">n_clusters = </span><span class="s4">4</span>
<span class="s1">y_lower = </span><span class="s4">10</span>
<span class="s1">plt.figure(figsize=(</span><span class="s4">10</span><span class="s1">, </span><span class="s4">6</span><span class="s1">))</span>
<span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(n_clusters):</span>
    <span class="s1">ith_cluster_silhouette_values = silhouette_vals[cluster_labels == i]</span>
    <span class="s1">ith_cluster_silhouette_values.sort()</span>
    <span class="s1">size_cluster_i = ith_cluster_silhouette_values.shape[</span><span class="s4">0</span><span class="s1">]</span>
    <span class="s1">y_upper = y_lower + size_cluster_i</span>

    <span class="s1">plt.fill_betweenx(np.arange(y_lower, y_upper),</span>
                      <span class="s4">0</span><span class="s1">, ith_cluster_silhouette_values,</span>
                      <span class="s1">alpha=</span><span class="s4">0.7</span><span class="s1">)</span>
    <span class="s1">plt.text(-</span><span class="s4">0.05</span><span class="s1">, y_lower + </span><span class="s4">0.5 </span><span class="s1">* size_cluster_i, str(i))</span>
    <span class="s1">y_lower = y_upper + </span><span class="s4">10  </span><span class="s0"># 10 for spacing between clusters</span>

<span class="s1">plt.axvline(x=silhouette_avg, color=</span><span class="s3">&quot;red&quot;</span><span class="s1">, linestyle=</span><span class="s3">&quot;--&quot;</span><span class="s1">)</span>
<span class="s1">plt.title(</span><span class="s3">&quot;Silhouette Plot for KMeans Clustering&quot;</span><span class="s1">)</span>
<span class="s1">plt.xlabel(</span><span class="s3">&quot;Silhouette Coefficient&quot;</span><span class="s1">)</span>
<span class="s1">plt.ylabel(</span><span class="s3">&quot;Cluster&quot;</span><span class="s1">)</span>
<span class="s1">plt.show()</span>

<span class="s1">print(</span><span class="s3">&quot;Silhouette Score:&quot;</span><span class="s1">, silhouette_avg)</span>
<span class="s0">#%% md 
</span><span class="s1">Considering only '*innovative_products*', '*price*', '*single_portion*', and '*speed_n_simplicity*', the silhouette score rises from 0.22 to 0.36 with K=4. Removing 'price' pushes it a bit higher to 0.40—slightly better, but still low.  
 
Given that this modest improvement comes at the cost of discarding several other variables, it may not be worth to sacrifice so much information for such a small gain. For this reason, we decided to proceed with the upcoming analysis using our previous 4-cluster K-Means analysis complete with all the variables. 
</span><span class="s0">#%% md 
</span><span class="s1">--- 
 
 
### **2.5. Clusters Analysis** 
</span><span class="s0">#%% md 
</span><span class="s1">#### **2.5.1. Analysis of sample** 
 
For the final considerations, we proceed with analysis of characteristics of the sample in general. 
 
The variables included are the following: 
- **age**: the age range of the respondent. 
- **region**: the region in Italy where the respondent lives. 
- **location**: the type of area the respondent lives in, such as city centre or suburbs. 
- **gender**: the gender with which the respondent identifies. 
- **family_members**: the number of people in the respondent's household. 
- **job**: the respondent's profession. 
- **consumption_frequency**: how often the respondent consumes seafood. 
- **fish_type**: the type of seafood consumed most frequently at home. 
- **meal_preparation_time**: the average time spent preparing meals each day. 
- **grocery_responsibility**: how often the respondent is responsible for grocery shopping. 
- **grocery_place**: where the respondent most often purchases seafood. 
- **WTP_fish**: the amount the respondent is willing to spend on a high-quality seafood product. 
</span><span class="s0">#%% 
</span><span class="s1">descriptor_data = data.iloc[:,</span><span class="s4">1</span><span class="s1">:</span><span class="s4">16</span><span class="s1">]</span>
<span class="s1">descriptor_data = descriptor_data.drop(descriptor_data.columns[[</span><span class="s4">6</span><span class="s1">,</span><span class="s4">7</span><span class="s1">,</span><span class="s4">14</span><span class="s1">]], axis=</span><span class="s4">1</span><span class="s1">)</span>

<span class="s0"># rename columns</span>
<span class="s1">new_names = { </span><span class="s3">'Qual è la tua età?'</span><span class="s1">: </span><span class="s3">'age'</span><span class="s1">, </span>
              <span class="s3">'In che regione vivi?'</span><span class="s1">:</span><span class="s3">'region'</span><span class="s1">, </span>
              <span class="s3">'In che zona abiti?'</span><span class="s1">: </span><span class="s3">'location'</span><span class="s1">,</span>
       <span class="s3">'Con quale genere ti identifichi?' </span><span class="s1">: </span><span class="s3">'gender'</span><span class="s1">,</span>
       <span class="s3">'Da quanti componenti è formato il tuo nucleo familiare?' </span><span class="s1">: </span><span class="s3">'family_members'</span><span class="s1">,</span>
       <span class="s3">'Qual è la tua professione?'</span><span class="s1">: </span><span class="s3">'job'</span><span class="s1">,</span>
        <span class="s3">'Quanto spesso consumi prodotti ittici?'</span><span class="s1">: </span><span class="s3">'consumption_frequency'</span><span class="s1">,</span>
       <span class="s3">'Che tipo di prodotti ittici consumi più frequentemente in casa?'</span><span class="s1">: </span><span class="s3">'fish_type'</span><span class="s1">,</span>
       <span class="s3">'Quanto tempo dedichi mediamente alla preparazione dei pasti durante la giornata?' </span><span class="s1">: </span><span class="s3">'meal_preparation_time'</span><span class="s1">,</span>
       <span class="s3">'Solitamente ti occupi tu della spesa nella tua famiglia?'</span><span class="s1">: </span><span class="s3">'grocery_responsibility'</span><span class="s1">,</span>
       <span class="s3">'Dove acquisti più spesso i tuoi prodotti ittici? (Puoi selezionare più opzioni) '</span><span class="s1">: </span><span class="s3">'grocery_place'</span><span class="s1">,</span>
       <span class="s3">'Quanto saresti disposto a spendere per un prodotto ittico di alta qualità (es. burger di pesce gourmet)?'</span><span class="s1">: </span><span class="s3">'WTP_fish'</span>
    
<span class="s1">}</span>

<span class="s1">descriptor_data = descriptor_data.rename(columns=new_names)</span>
<span class="s1">descriptor_data</span>
<span class="s0">#%% 
# remove emoji</span>
<span class="s2">def </span><span class="s1">remove_emojis(text):</span>
    <span class="s2">return </span><span class="s1">re.sub(</span><span class="s3">r'[\U00010000-\U0010ffff]'</span><span class="s1">, </span><span class="s3">''</span><span class="s1">, text)</span>

<span class="s1">descriptor_data = descriptor_data.map(</span><span class="s2">lambda </span><span class="s1">x: remove_emojis(x) </span><span class="s2">if </span><span class="s1">isinstance(x, str) </span><span class="s2">else </span><span class="s1">x)</span>
<span class="s0">#%% md 
</span><span class="s1">Let's first examine the distribution of these variables across the entire dataset.  
 
This allows us to analyze the sample and better understand the underlying patterns in the data. 
</span><span class="s0">#%% 
# Plot bar chart for each variable</span>
<span class="s1">sns.set(style=</span><span class="s3">&quot;whitegrid&quot;</span><span class="s1">)</span>

<span class="s2">def </span><span class="s1">plot_bar_grid(df, columns, cols=</span><span class="s4">1</span><span class="s1">, explode_columns=</span><span class="s2">None</span><span class="s1">, delimiter=</span><span class="s3">','</span><span class="s1">):</span>
    <span class="s2">if </span><span class="s1">explode_columns </span><span class="s2">is None</span><span class="s1">:</span>
        <span class="s1">explode_columns = []</span>

    <span class="s1">processed_df = df.copy()</span>
    <span class="s2">for </span><span class="s1">column </span><span class="s2">in </span><span class="s1">explode_columns:</span>
        <span class="s2">if </span><span class="s1">column </span><span class="s2">in </span><span class="s1">processed_df.columns:</span>
            <span class="s1">processed_df = processed_df.dropna(subset=[column])</span>
            <span class="s1">processed_df[column] = processed_df[column].str.split(delimiter)</span>
            <span class="s1">processed_df = processed_df.explode(column)</span>

    <span class="s1">total_plots = len(columns)</span>
    <span class="s1">rows = (total_plots + cols - </span><span class="s4">1</span><span class="s1">) // cols </span>

    <span class="s1">fig, axes = plt.subplots(rows, cols, figsize=(</span><span class="s4">20</span><span class="s1">, rows * </span><span class="s4">6</span><span class="s1">))  </span>
    <span class="s1">axes = axes.flatten()</span>

    <span class="s2">for </span><span class="s1">i, column </span><span class="s2">in </span><span class="s1">enumerate(columns):</span>
        <span class="s2">if </span><span class="s1">column </span><span class="s2">in </span><span class="s1">processed_df.columns:</span>
            <span class="s1">sns.countplot(data=processed_df, x=column, hue=column, palette=</span><span class="s3">'Set2'</span><span class="s1">, legend=</span><span class="s2">False</span><span class="s1">, ax=axes[i])</span>
            <span class="s1">axes[i].set_title(</span><span class="s3">f'Distribution of </span><span class="s5">{</span><span class="s1">column</span><span class="s5">}</span><span class="s3">'</span><span class="s1">, fontsize=</span><span class="s4">17</span><span class="s1">)</span>
            <span class="s1">axes[i].tick_params(axis=</span><span class="s3">'x'</span><span class="s1">, rotation=</span><span class="s4">30</span><span class="s1">, labelsize=</span><span class="s4">13</span><span class="s1">) </span>
            <span class="s1">axes[i].tick_params(axis=</span><span class="s3">'y'</span><span class="s1">, labelsize=</span><span class="s4">13</span><span class="s1">) </span>

    <span class="s2">for </span><span class="s1">j </span><span class="s2">in </span><span class="s1">range(len(columns), len(axes)):</span>
        <span class="s1">fig.delaxes(axes[j])</span>

    <span class="s1">plt.tight_layout()</span>
    <span class="s1">plt.subplots_adjust(hspace=</span><span class="s4">1</span><span class="s1">) </span>
    <span class="s1">plt.show()</span>


<span class="s1">columns_to_plot = descriptor_data.columns</span>
<span class="s1">plot_bar_grid(</span>
    <span class="s1">descriptor_data,</span>
    <span class="s1">columns_to_plot,</span>
    <span class="s1">cols=</span><span class="s4">1</span><span class="s1">, </span>
    <span class="s1">explode_columns=[</span><span class="s3">'grocery_place'</span><span class="s1">], </span>
    <span class="s1">delimiter=</span><span class="s3">';' </span>
<span class="s1">)</span>
<span class="s0">#%% md 
</span><span class="s1">**Demographic Characteristics:** 
- **age:** the majority of respondents are in the 18-25 years range, making up nearly half of the sample. The second-largest group is the 46-60 years range, followed by 26-35 years. The older age groups (60-70 years and 70+ years) represent a smaller proportion of the sample. This suggests that the sample may be skewed towards younger individuals. 
- **region:** the distribution suggests a concentration of respondents in certain regions, particularly in Lombardia and Puglia. Several other regions like Campania, Liguria, and others represent a smaller proportion, and some regions are underrepresented with only a handful of respondents. This may impact the generalizability of the findings if the sample does not reflect the national distribution evenly. 
- **location:** this suggests a relatively balanced distribution between urban (city center and suburbs) and more rural areas, although the suburban areas slightly dominate the sample. This could indicate that the survey captures a mix of urban and non-urban experiences but leans slightly towards suburban living. 
- **gender:** this distribution indicates a strong skew towards female respondents. 
- **family_members:** this suggests that most respondents come from medium to large households, with a relatively smaller group of single-person households. 
- **job:** the survey primarily captures perspectives from the working and student populations, with a smaller representation from retirees and the unemployed. The distribution indicates a fairly typical demographic spread of working-age individuals and students, though the sample is more heavily skewed towards those in employment. 
 
 
**Habits:** 
- **consumption_frequency:** seafood is a relatively regular part of many respondents' diets, with a significant number of people consuming it weekly or more frequently. However, there is still a notable portion of the sample that consumes seafood infrequently. 
- **fish_type:** this distribution indicates a strong preference for traditional forms of seafood, with fresh and frozen options being the most commonly consumed types. 
- **meal_preparation:** only a small portion spends more than an hour preparing meals, while the majority spends an average time of around 30 minutes. 
- **grocery_responsibility:** more than half of the people take care of grocery shopping, and only a small portion never do, indicating that the data collected comes from individuals who actually purchase fish, meaning potential customers. 
- **grocery_place:** the majority buy from supermarkets, while only a smaller portion purchase from specialized shops or local markets, making them not potential customers since our company sells frozen fish. 
- **WTP_fish:** there is a fairly even distribution across all the proposed ranges. 
 
While obtaining a sample through **convenience sampling** allowed us to get a rapid collection of useful data, it presents significant limitations in terms of the representativeness of the general population. 
 
First of all, we can see the over-representation of certain categories, indeed specific groups, such as individuals aged 18–25 and residents of Lombardia and Puglia. This may distort the results and limit the generalizability of the conclusions. 
 
In addiction to this, there is a slight **gender imbalance**, with the predominance of female respondents (over 60%) that may introduce bias. 
 
We can conclude that we do not have a perfectly representative sample of the entire population, but we do have votes for some categories, and if the limitations are properly taken into account, we could still conduct a good analysis. 
</span><span class="s0">#%% md 
</span><span class="s1">--- 
 
 
#### **2.5.2. Analysis of clusters** 
 
After analyzing demographic information, we now examine the specific characteristics of each cluster to identify patterns and differences. 
</span><span class="s0">#%% 
</span><span class="s1">descriptor_data[</span><span class="s3">'Cluster_K2'</span><span class="s1">] = final_segmentation_data_2[</span><span class="s3">'Cluster_KMeans'</span><span class="s1">]</span>
<span class="s1">descriptor_data[</span><span class="s3">'Cluster_K4'</span><span class="s1">] = final_segmentation_data_4[</span><span class="s3">'Cluster_KMeans'</span><span class="s1">]</span>
<span class="s0">#%% md 
</span><span class="s1">To gain some insights, we created **pie charts of the distribution of variables across each cluster**. 
</span><span class="s0">#%% 
</span><span class="s1">clusters = descriptor_data[</span><span class="s3">'Cluster_K2'</span><span class="s1">].unique()</span>
<span class="s1">variables = [col </span><span class="s2">for </span><span class="s1">col </span><span class="s2">in </span><span class="s1">descriptor_data.columns </span><span class="s2">if </span><span class="s1">col </span><span class="s2">not in </span><span class="s1">[</span><span class="s3">'Cluster_K2'</span><span class="s1">, </span><span class="s3">'respondent_id'</span><span class="s1">]]</span>

<span class="s1">num_clusters = len(clusters)</span>
<span class="s1">num_variables = len(variables)</span>
<span class="s1">fig, axs = plt.subplots(num_clusters, num_variables, figsize=(</span><span class="s4">5 </span><span class="s1">* num_variables, </span><span class="s4">5 </span><span class="s1">* num_clusters)) </span>

<span class="s2">if </span><span class="s1">num_clusters == </span><span class="s4">1 </span><span class="s2">or </span><span class="s1">num_variables == </span><span class="s4">1</span><span class="s1">:</span>
    <span class="s1">axs = np.array(axs).reshape(num_clusters, num_variables)</span>

<span class="s0"># Plot pie charts for each cluster and variable</span>
<span class="s2">for </span><span class="s1">i, cluster </span><span class="s2">in </span><span class="s1">enumerate(clusters):</span>
    <span class="s1">cluster_data = descriptor_data[descriptor_data[</span><span class="s3">'Cluster_K2'</span><span class="s1">] == cluster]</span>
    <span class="s2">for </span><span class="s1">j, var </span><span class="s2">in </span><span class="s1">enumerate(variables):</span>
        <span class="s1">counts = cluster_data[var].value_counts()</span>
        <span class="s1">ax = axs[i, j] </span><span class="s2">if </span><span class="s1">num_clusters &gt; </span><span class="s4">1 </span><span class="s2">else </span><span class="s1">axs[j]</span>
        <span class="s1">ax.pie(counts, labels=counts.index, autopct=</span><span class="s3">'%1.1f%%'</span><span class="s1">, startangle=</span><span class="s4">90</span><span class="s1">)</span>
        <span class="s1">ax.set_title(</span><span class="s3">f'Cluster </span><span class="s5">{</span><span class="s1">cluster</span><span class="s5">} </span><span class="s3">- </span><span class="s5">{</span><span class="s1">var</span><span class="s5">}</span><span class="s3">'</span><span class="s1">)</span>

<span class="s1">plt.tight_layout()</span>
<span class="s1">plt.show()</span>
<span class="s0">#%% md 
</span><span class="s1">**Analysis with K=2**: 
- **Cluster 1:** this cluster highly values freshness, origin, and unprocessed products. They also place significant importance on sustainable packaging and sources, suggesting a strong environmental awareness. Less importance is placed on innovative products, single portions, and the speed and simplicity of product preparation. This indicates a preference for traditional products that do not necessarily require quick preparation or come pre-packaged in single servings.  
Predominantly female under 35 years old, a large portion (42,2%) spend 30-60 minutes on meal preparation. Mostly, they buy fish at supermarkets, but there are also modest percentages who purchase from fish shops and local markets. 
- **Cluster 2:** low importance is placed on attributes like freshness, origin and sustainable aspects. This cluster seems to prioritize convenience and price over other factors. 
More gender-balanced, mainly students, spend less time preparing meals, consume fish less frequently, mainly buy fish products at supermarkets. 
</span><span class="s0">#%% 
</span><span class="s1">clusters = descriptor_data[</span><span class="s3">'Cluster_K4'</span><span class="s1">].unique()</span>
<span class="s1">variables = [col </span><span class="s2">for </span><span class="s1">col </span><span class="s2">in </span><span class="s1">descriptor_data.columns </span><span class="s2">if </span><span class="s1">col </span><span class="s2">not in </span><span class="s1">[</span><span class="s3">'Cluster_K4'</span><span class="s1">, </span><span class="s3">'respondent_id'</span><span class="s1">]]</span>

<span class="s1">num_clusters = len(clusters)</span>
<span class="s1">num_variables = len(variables)</span>
<span class="s1">fig, axs = plt.subplots(num_clusters, num_variables, figsize=(</span><span class="s4">5 </span><span class="s1">* num_variables, </span><span class="s4">5 </span><span class="s1">* num_clusters))</span>

<span class="s2">if </span><span class="s1">num_clusters == </span><span class="s4">1 </span><span class="s2">or </span><span class="s1">num_variables == </span><span class="s4">1</span><span class="s1">:</span>
    <span class="s1">axs = np.array(axs).reshape(num_clusters, num_variables)</span>

<span class="s0"># Plot pie charts for each cluster and variable</span>
<span class="s2">for </span><span class="s1">i, cluster </span><span class="s2">in </span><span class="s1">enumerate(clusters):</span>
    <span class="s1">cluster_data = descriptor_data[descriptor_data[</span><span class="s3">'Cluster_K4'</span><span class="s1">] == cluster]</span>
    <span class="s2">for </span><span class="s1">j, var </span><span class="s2">in </span><span class="s1">enumerate(variables): </span>
        <span class="s1">counts = cluster_data[var].value_counts()</span>
        <span class="s1">ax = axs[i, j] </span><span class="s2">if </span><span class="s1">num_clusters &gt; </span><span class="s4">1 </span><span class="s2">else </span><span class="s1">axs[j]</span>
        <span class="s1">ax.pie(counts, labels=counts.index, autopct=</span><span class="s3">'%1.1f%%'</span><span class="s1">, startangle=</span><span class="s4">90</span><span class="s1">)</span>
        <span class="s1">ax.set_title(</span><span class="s3">f'Cluster </span><span class="s5">{</span><span class="s1">cluster</span><span class="s5">} </span><span class="s3">- </span><span class="s5">{</span><span class="s1">var</span><span class="s5">}</span><span class="s3">'</span><span class="s1">)</span>

<span class="s1">plt.tight_layout()</span>
<span class="s1">plt.show()</span>
<span class="s0">#%% md 
</span><span class="s1">**Analysis with K=4** 
- **Cluster 1:**  this group highly values **freshness**, **price**, and **unprocessed products**, but shows less interest in **innovative products** and **sustainable packaging**. Primarily composed of younger individuals aged **18-35**, with a balanced mix of workers and students. They mainly shop at supermarkets and spend little time on meal preparation, often opting for ready-made or frozen meals. Despite their interest in freshness, their budget constraints and lifestyle drive them to prioritize affordability over quality, highlighting a potential aspiration for fresh fish that is not always met in practice. 
 
- **Cluster 2:**  this group exhibits the **lowest interest across all attributes**, showing little importance for **freshness**, **origin**, or **sustainability**, and consuming the least amount of fish. The majority are younger individuals, mainly under **35 years old**, who spend minimal time cooking and tend to prefer ready-made products. This group represents the least engaged segment with minimal attachment to fish consumption and its attributes. 
 
- **Cluster 3:**  this is the **most demanding group**, placing high importance on **freshness**, **origin**, and **sustainability** across all variables. A significant portion of this cluster is women (**82.7%**), with a diverse age distribution but the highest representation of people aged **40-60 years**. Many come from **Puglia**, a region likely offering greater access to fresh fish. This group spends the most time cooking and exhibits strong preferences for high-quality and traditional fish products, reflecting their commitment to quality meals. 
 
- **Cluster 4:**  this group values **freshness**, **origin**, **unprocessed products**, and **sustainability**, but shows less interest in **innovative products**, **single servings**, and the **speed and simplicity of preparation**. The cluster has a diverse age distribution and is predominantly composed of workers. A large portion spends **30-60 minutes** on meal preparation (**46.6%**) and doesn't show a clear preference for buying places. They represent a more traditional segment that prioritizes quality over convenience. 
 
 
A more detailed analysis of the clusters reveals some **inconsistencies**, possibly due to respondents’ biases and bad answering.  
 
For example, the first cluster places great importance on freshness but is also very sensitive to price, which is somewhat **contradictory**. In fact, when examining their willingness to pay for fish, only 19% would be willing to spend more than 10 euros for a quality product, and about 70% purchase at supermarkets. This suggests that they would not be inclined to shop at markets or specialty stores for higher quality products.  
 
Similarly, Cluster 3 places high importance on almost all variables (freshness, origin, nutritional balance, sustainability), but also on price, which is contradictory as well because purchasing a fish product that combines all these features would necessarily be quite expensive. Again, the pie chart shows that most of them shop at supermarkets, and only 16% would be willing to spend more than 10 euros. 
 
 
**Comparison Between \(k=2\) and \(k=4\):** 
- When using \(k=2\), the clusters separate into two broad groups: one with **high attribute values** and another with **low attribute values**. Pie charts show that clusters 2 and 3 from \(k=4\) completely correspond to a specific cluster in \(k=2\), while clusters 1 and 4 don't have a clear correspondence.  
- However, \(k=4\) provides a more subtle understanding of specific interests, while \(k=2\) overly generalizes the segments. 
- We decided to proceed with \(k=4\) because it offers more granular insights into consumer preferences, which are valuable for identifying specific target audiences. 
 
 
Given that our company specializes in frozen products, we could approach each cluster with tailored strategies: 
 
- **Cluster 3:** This group shows a strong preference for fresh products, which makes them less aligned with our core offerings. 
 
- **Clusters 1 and 2:** These clusters are more inclined toward convenience-oriented products. They are likely to be interested in ready-made, easy-to-cook frozen meals, which align well with our product offerings. Emphasizing the speed and simplicity of preparation will be key to targeting these segments effectively. 
 
- **Cluster 4:** While this cluster prioritizes traditional products and spends more time cooking, they do not completely exclude frozen products. By focusing on high-quality frozen options that align with traditional cooking styles (e.g., unprocessed or minimally processed products), we can position our offerings as complementary to their preferences. 
</span><span class="s0">#%% md 
</span><span class="s1">--- 
 
 
## **3. Classification** 
</span><span class="s0">#%% md 
</span><span class="s1">As the final step of this analysis, we aim to build predictive classification models that can **assign potential consumers to the appropriate clusters** based on their descriptive data. This approach allows us to generalize our clustering insights, transforming them into actionable tools for predicting the behavior and the preferences of future consumers. 
 
To assess the robustness of the identified clusters and their predictive power, we will evaluate different classification models, such as Random Forest and Multinomial Logistic Regression. By comparing their performance, we seek to **determine whether the clusters effectively capture meaningful distinctions** within the data and provide a reliable foundation for predictive analysis. 
</span><span class="s0">#%% md 
</span><span class="s1">### **3.1. Multinomial Logit model** 
</span><span class="s0">#%% md 
</span><span class="s1">We first implemented a Multinomial Logistic Regression model to **predict consumer cluster membership** based on key demographic and socio-economic attributes. The independent variables include age, region, location, gender, family members, and job type, with cluster assignments from K-means (K=4) as the categorical outcome.  
 
Before fitting the model, we performed minor data preprocessing, including recoding specific location labels for consistency. 
</span><span class="s0">#%% 
</span><span class="s1">df = descriptor_data[[</span><span class="s3">'age'</span><span class="s1">, </span><span class="s3">'region'</span><span class="s1">, </span><span class="s3">'location'</span><span class="s1">, </span><span class="s3">'gender'</span><span class="s1">, </span><span class="s3">'family_members'</span><span class="s1">, </span><span class="s3">'job'</span><span class="s1">, </span><span class="s3">'Cluster_K4'</span><span class="s1">]].copy()</span>
<span class="s1">df[</span><span class="s3">&quot;location&quot;</span><span class="s1">] = df[</span><span class="s3">&quot;location&quot;</span><span class="s1">].replace(</span><span class="s3">&quot;Esterno (es. campagna/costiera/montagna...)&quot;</span><span class="s1">, </span><span class="s3">&quot;Esterno&quot;</span><span class="s1">)</span>
<span class="s1">df</span>
<span class="s0">#%% 
</span><span class="s1">print((df[</span><span class="s3">'gender'</span><span class="s1">] == </span><span class="s3">'Altro'</span><span class="s1">).sum())</span>
<span class="s0">#%% md 
</span><span class="s1">Since only two people chose &quot;Other&quot; as gender, we chose to delete this category to avoid multicollinearity.  
</span><span class="s0">#%% 
</span><span class="s1">df_logit = df[df[</span><span class="s3">'gender'</span><span class="s1">] != </span><span class="s3">'Altro'</span><span class="s1">]</span>
<span class="s1">df_logit</span>
<span class="s0">#%% 
</span><span class="s1">category_counts = df_logit.nunique()</span>
<span class="s1">category_counts</span>
<span class="s0">#%% md 
</span><span class="s1">After calculating the count of unique values for each column, we decided to exclude the category 'region' as well because it had too many elements (15) that could have actually created issues in applying the model. 
</span><span class="s0">#%% 
</span><span class="s1">df_reduced = df_logit[[</span><span class="s3">'age'</span><span class="s1">, </span><span class="s3">'location'</span><span class="s1">, </span><span class="s3">'gender'</span><span class="s1">, </span><span class="s3">'family_members'</span><span class="s1">, </span><span class="s3">'job'</span><span class="s1">, </span><span class="s3">'Cluster_K4'</span><span class="s1">]]</span>
<span class="s1">df_reduced= pd.get_dummies(df_reduced, drop_first=</span><span class="s2">True</span><span class="s1">, dtype=int)</span>

<span class="s0"># renaming columns</span>
<span class="s1">df_reduced.columns = df_reduced.columns.str.replace(</span><span class="s3">r&quot;[^\w]&quot;</span><span class="s1">, </span><span class="s3">&quot;_&quot;</span><span class="s1">, regex=</span><span class="s2">True</span><span class="s1">)</span>
<span class="s0">#%% md 
</span><span class="s1">After that, we calculated the Variance Inflation Factor (VIF) to check for **multicollinearity** among the variables. Most of the variables have a VIF below 5, which indicates that multicollinearity is pretty low. 
</span><span class="s0">#%% 
</span><span class="s1">df_vif = df_reduced.drop(columns=[</span><span class="s3">'Cluster_K4'</span><span class="s1">])</span>
<span class="s1">vif_data = pd.DataFrame()</span>
<span class="s1">vif_data[</span><span class="s3">&quot;Feature&quot;</span><span class="s1">] = df_vif.columns</span>
<span class="s1">vif_data[</span><span class="s3">&quot;VIF&quot;</span><span class="s1">] = [variance_inflation_factor(df_vif.values, i) </span><span class="s2">for </span><span class="s1">i </span><span class="s2">in </span><span class="s1">range(df_vif.shape[</span><span class="s4">1</span><span class="s1">])]</span>

<span class="s1">print(vif_data)</span>
<span class="s0">#%% 
</span><span class="s1">independent_vars = df_reduced.columns.difference([</span><span class="s3">'Cluster_K4'</span><span class="s1">]).tolist()</span>
<span class="s1">formula = </span><span class="s3">&quot;Cluster_K4 ~ &quot; </span><span class="s1">+ </span><span class="s3">&quot; + &quot;</span><span class="s1">.join(independent_vars)</span>

<span class="s1">model1 = smf.mnlogit(formula=formula, data=df_reduced).fit()</span>
<span class="s1">model1.summary()</span>
<span class="s0">#%% md 
</span><span class="s1">The model failed to converge to an optimal solution and the pseudo-R² is really low. This could be due to having too many categorical levels and too few observations. Therefore, we attempted to reduce the number of independent variables. 
</span><span class="s0">#%% 
</span><span class="s1">df_reduced = df[[</span><span class="s3">'gender'</span><span class="s1">, </span><span class="s3">'location'</span><span class="s1">, </span><span class="s3">'family_members'</span><span class="s1">, </span><span class="s3">'job'</span><span class="s1">, </span><span class="s3">'Cluster_K4'</span><span class="s1">]]</span>
<span class="s1">df_reduced = pd.get_dummies(df_reduced, drop_first=</span><span class="s2">True</span><span class="s1">, dtype=int)</span>

<span class="s0"># renaming columns</span>
<span class="s1">df_reduced.columns = df_reduced.columns.str.replace(</span><span class="s3">r&quot;[^\w]&quot;</span><span class="s1">, </span><span class="s3">&quot;_&quot;</span><span class="s1">, regex=</span><span class="s2">True</span><span class="s1">)</span>

<span class="s0"># fitting model</span>
<span class="s1">independent_vars = df_reduced.columns.difference([</span><span class="s3">'Cluster_K4'</span><span class="s1">]).tolist()</span>
<span class="s1">formula = </span><span class="s3">&quot;Cluster_K4 ~ &quot; </span><span class="s1">+ </span><span class="s3">&quot; + &quot;</span><span class="s1">.join(independent_vars)</span>

<span class="s1">model2 = smf.mnlogit(formula=formula, data=df_reduced).fit()</span>
<span class="s1">model2.summary()</span>
<span class="s0">#%% 
</span><span class="s1">df_reduced = df[[</span><span class="s3">'gender'</span><span class="s1">, </span><span class="s3">'family_members'</span><span class="s1">, </span><span class="s3">'job'</span><span class="s1">, </span><span class="s3">'Cluster_K4'</span><span class="s1">]]</span>
<span class="s1">df_reduced = pd.get_dummies(df_reduced, drop_first=</span><span class="s2">True</span><span class="s1">, dtype=int)</span>

<span class="s0"># renaming columns</span>
<span class="s1">df_reduced.columns = df_reduced.columns.str.replace(</span><span class="s3">r&quot;[^\w]&quot;</span><span class="s1">, </span><span class="s3">&quot;_&quot;</span><span class="s1">, regex=</span><span class="s2">True</span><span class="s1">)</span>

<span class="s0"># fitting model</span>
<span class="s1">independent_vars = df_reduced.columns.difference([</span><span class="s3">'Cluster_K4'</span><span class="s1">]).tolist()</span>
<span class="s1">formula = </span><span class="s3">&quot;Cluster_K4 ~ &quot; </span><span class="s1">+ </span><span class="s3">&quot; + &quot;</span><span class="s1">.join(independent_vars)</span>

<span class="s1">model3 = smf.mnlogit(formula=formula, data=df_reduced).fit()</span>
<span class="s1">model3.summary()</span>
<span class="s0">#%% md 
</span><span class="s1">Even reducing the number of the variables, the model still struggled to converge and its performance deteriorated even more, with pseudo-R² slightly lower than before. 
</span><span class="s0">#%% md 
</span><span class="s1">Then, we procedeed in representing the confusion matrix based on the first model. 
</span><span class="s0">#%% 
</span><span class="s1">PredTable = model1.pred_table()</span>
<span class="s1">cluster_labels = np.arange(</span><span class="s4">1</span><span class="s1">, PredTable.shape[</span><span class="s4">0</span><span class="s1">] + </span><span class="s4">1</span><span class="s1">)</span>
<span class="s1">ConfusionMatrixDisplay(confusion_matrix=PredTable, display_labels=cluster_labels).plot()</span>

<span class="s1">plt.title(</span><span class="s3">&quot;Confusion Matrix&quot;</span><span class="s1">)</span>
<span class="s1">plt.show()</span>
<span class="s0">#%% md 
</span><span class="s1">**Cluster 4 has the best classification performance** (56 correct predictions).  
Cluster 2 instead has the worst performance (only 2 correctly classified cases). 
 
We can conclude that there is significant confusion between certain clusters, indicating that the groups might have overlapping features. 
</span><span class="s0">#%% md 
</span><span class="s1">### **3.2. Random Forest** 
</span><span class="s0">#%% md 
</span><span class="s1">Given the poor performance of the Multinomial Logit model, we explored an alternative approach using a Random Forest model, a more flexible method that may improve predictive accuracy. 
</span><span class="s0">#%% 
# apply One-Hot Encoding to categorical variables</span>
<span class="s1">df_encoded = pd.get_dummies(df, columns=[</span><span class="s3">&quot;age&quot;</span><span class="s1">, </span><span class="s3">&quot;region&quot;</span><span class="s1">, </span><span class="s3">&quot;location&quot;</span><span class="s1">, </span><span class="s3">&quot;gender&quot;</span><span class="s1">, </span><span class="s3">&quot;family_members&quot;</span><span class="s1">, </span><span class="s3">&quot;job&quot;</span><span class="s1">], drop_first=</span><span class="s2">True</span><span class="s1">)</span>

<span class="s1">X = df_encoded.drop(columns=[</span><span class="s3">&quot;Cluster_K4&quot;</span><span class="s1">])</span>
<span class="s1">y = df_encoded[</span><span class="s3">&quot;Cluster_K4&quot;</span><span class="s1">]</span>

<span class="s0"># split dataset into training and test set</span>
<span class="s1">X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=</span><span class="s4">0.2</span><span class="s1">, random_state=</span><span class="s4">42</span><span class="s1">)</span>

<span class="s0"># Random Forest model</span>
<span class="s1">rf = RandomForestClassifier(n_estimators=</span><span class="s4">30</span><span class="s1">, random_state=</span><span class="s4">42</span><span class="s1">)</span>
<span class="s1">rf.fit(X_train, y_train)</span>

<span class="s0"># predictions</span>
<span class="s1">y_pred = rf.predict(X_test)</span>

<span class="s0"># model accuracy</span>
<span class="s1">accuracy = accuracy_score(y_test, y_pred)</span>
<span class="s1">accuracy</span>
<span class="s0">#%% 
# classification report</span>
<span class="s1">report = classification_report(y_test, y_pred, output_dict=</span><span class="s2">True</span><span class="s1">)</span>
<span class="s1">report_df = pd.DataFrame(report).transpose()</span>
<span class="s1">print(report_df)</span>
<span class="s0">#%% md 
</span><span class="s1">The Random Forest model achieved an accuracy of 37%, indicating that the model has limited predictive performance. 
- **Cluster 1:** low precision (0.25) and low recall (0.2857) indicate that the model struggles to correctly classify instances belonging to this cluster. The F1-score (0.2667) suggests that predictions for this class are not reliable. 
- **Cluster 2:** high precision (0.75) but low recall (0.2143) means the model correctly identifies cluster 2 when it does so, but it often fails to classify many true cluster 2 instances. This suggests the model is overly selective, likely favoring other clusters. 
- **Cluster 3:** moderate precision (0.4615) and moderate recall (0.4000) indicate a balanced, but not strong, classification performance. The F1-score (0.4286) shows that while predictions are better than for cluster 1, there is room for improvement. 
- **Cluster 4:** higher recall (0.5263) means that many instances of this cluster are correctly identified. Lower precision (0.3448) suggests that the model also misclassifies some other cluster instances as cluster 4. 
</span><span class="s0">#%% 
</span><span class="s1">feature_importances = rf.feature_importances_</span>
<span class="s1">feature_importance_df = pd.DataFrame({</span><span class="s3">'Feature'</span><span class="s1">: X.columns, </span><span class="s3">'Importance'</span><span class="s1">: feature_importances})</span>
<span class="s1">feature_importance_df = feature_importance_df.sort_values(by=</span><span class="s3">'Importance'</span><span class="s1">, ascending=</span><span class="s2">False</span><span class="s1">)</span>

<span class="s1">plt.figure(figsize=(</span><span class="s4">10</span><span class="s1">,</span><span class="s4">8</span><span class="s1">))</span>
<span class="s1">plt.barh(feature_importance_df[</span><span class="s3">'Feature'</span><span class="s1">], feature_importance_df[</span><span class="s3">'Importance'</span><span class="s1">])</span>
<span class="s1">plt.xlabel(</span><span class="s3">'Feature Importance'</span><span class="s1">)</span>
<span class="s1">plt.ylabel(</span><span class="s3">'Feature'</span><span class="s1">)</span>
<span class="s1">plt.title(</span><span class="s3">'Feature importance'</span><span class="s1">)</span>
<span class="s1">plt.gca().invert_yaxis()</span>
<span class="s1">plt.show()</span>
<span class="s0">#%% md 
</span><span class="s1">After computing feature importance, we removed variables with low relevance (&lt;0.1) and retrained the model to evaluate potential performance improvements. 
</span><span class="s0">#%% 
# Set threshold for feature importance (remove features with importance &lt; 0.01)</span>
<span class="s1">threshold = </span><span class="s4">0.01</span>
<span class="s1">important_features = feature_importance_df[feature_importance_df[</span><span class="s3">'Importance'</span><span class="s1">] &gt;= threshold][</span><span class="s3">'Feature'</span><span class="s1">]</span>
<span class="s1">X_reduced = X[important_features]</span>

<span class="s0"># split dataset into training and test set</span>
<span class="s1">X_train_red, X_test_red, y_train_red, y_test_red = train_test_split(X_reduced, y, test_size=</span><span class="s4">0.2</span><span class="s1">, random_state=</span><span class="s4">42</span><span class="s1">)</span>

<span class="s0"># retrain the Random Forest model with reduced features</span>
<span class="s1">rf_reduced = RandomForestClassifier(n_estimators=</span><span class="s4">30</span><span class="s1">, random_state=</span><span class="s4">42</span><span class="s1">)</span>
<span class="s1">rf_reduced.fit(X_train_red, y_train_red)</span>

<span class="s0"># predictions</span>
<span class="s1">y_pred_red = rf_reduced.predict(X_test_red)</span>

<span class="s0"># accuracy</span>
<span class="s1">accuracy_reduced = accuracy_score(y_test_red, y_pred_red)</span>
<span class="s1">print(accuracy_reduced)</span>

<span class="s0"># classification report</span>
<span class="s1">report_reduced = classification_report(y_test_red, y_pred_red, output_dict=</span><span class="s2">True</span><span class="s1">)</span>
<span class="s1">report_reduced_df = pd.DataFrame(report_reduced).transpose()</span>
<span class="s1">print(report_reduced_df)</span>
<span class="s0">#%% md 
</span><span class="s1">After removing the low-importance features, the model was retrained, and the new accuracy is **33.87%**. This is slightly lower than the previous accuracy (**37%**), which suggests that some of the removed features might have had a small but positive impact. 
</span><span class="s0">#%% md 
</span><span class="s1">The low performance of both **Random Forest** and **Multinomial Logit** may be attributed to poorly defined clusters rather than model inefficiency.  If the original clustering (Cluster_K4) does not capture meaningful differences in the data, classification models will struggle to learn distinct patterns. In such cases, poor separation between clusters leads to significant overlap, making it difficult for any model to accurately predict class labels. 
</span><span class="s0">#%% md 
</span><span class="s1">--- 
 
 
## **4. Conclusions and Limitations** 
 
**Bias and Reliability Issues:** 
1. **Freshness Attribute:** the question about freshness appears to focus more on a general interest rather than the specific context of purchase. This could lead to inflated ratings, and we suggest reformulating the question for future surveys to better capture actionable insights. 
 
2. **Sustainability Attribute:** the responses on sustainability might be biased due to its status as a &quot;sensitive&quot; or &quot;hot topic&quot; for customers. Some respondents may have felt compelled to give high ratings, even if their behavior does not align with these preferences. This is evident from the highly skewed distribution of ratings, with a majority giving top scores (5). As a result, clusters may not be entirely reliable or reflective of true preferences. 
 
3. **Correlated Attributes:** certain attributes, such as **speed** and **simplicity of preparation**, as well as different dimensions of sustainability (e.g., **packaging**, **sustainable fishing**, and **low-impact environment**), are highly correlated. Reformulating these attributes or combining them into broader categories could improve the clarity of future analyses. 
 
4. **Price attribute:** the **'price'** attribute showed contradictions: while price sensitivity was highlighted as important, many respondents also indicated a willingness to spend more for higher quality. Reformulating this as **'willingness to pay more for quality'** could provide more actionable insights. 
 
5. **Innovative products attribute**: the attribute **'innovative_products'** stood out due to its relatively well-distributed responses. This makes it a valuable metric for understanding how to create new products and cater to specific consumer segments. 
 
 
**Limitations of the Clustering Analysis:** 
1. **Inefficiency in Clustering:** the clustering process showed weak separation between certain groups, indicating that the clusters are not fully robust or reliable. This may stem from biases in the data, the choice of attributes, a sample that is not sufficiently representative or inherent overlaps in consumer preferences. 
2. **Limited Reliability for Strategy:** given the inefficiency and potential biases, the clustering results cannot be fully relied upon for a completely data-driven strategy. While the analysis provides a general idea of segment characteristics, decisions should not be based solely on these clusters. Instead, we recommend focusing on the **most relevant cluster for specific goals** and combining these insights with qualitative research or additional data validation. 
      
These findings underline the need to refine survey design, improve clustering methods, and carefully interpret results to ensure a robust and reliable segmentation strategy. While this analysis provides useful initial insights, future iterations should address these limitations for more actionable outcomes.</span></pre>
</body>
</html>